{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthewleechen/woodcroft_patents/blob/main/notebooks/layout_detect_patents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook was designed for training in Google Colab Pro. It is **not** recommended to run this notebook on the Colab free plan. This notebook's training loop was originally run using Colab Pro on 1 Nvidia A100 (40GB) GPU. Training is both compute and time-intensive: the process consumed approximately 15-20 compute credits per hour, and the Faster-RCNN and Mask-RCNN (ResNet-50 backbone) models took approximately one full day (~12 hours) to train to 100,000 iterations. This is going to vary significantly depending on the quality of your input images that you need to load into GPU memory: I compressed them to 20% of their original size. You can also run this locally on a virtual machine or server, but carefully check for dependencies.\n",
        "\n",
        "This notebook uses the [Detectron2](https://github.com/facebookresearch/detectron2) library for object detection and instance segmentation from Facebook AI Research, the [Google Cloud Vision API](https://cloud.google.com/vision/docs) and the [LayoutParser](https://layout-parser.github.io) toolkit. It also borrows heavily from the [layout model training directory](https://github.com/Layout-Parser/layout-model-training) from LayoutParser."
      ],
      "metadata": {
        "id": "FV0l0XurJWMu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Prepare labelled data and directories**"
      ],
      "metadata": {
        "id": "TBh_Z6RLrL1N"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7agvGgfdLFU5"
      },
      "outputs": [],
      "source": [
        "# Clone forked layout-model-training repo from Layout-Parser\n",
        "! git clone https://github.com/matthewleechen/layout-model-training\n",
        "! cd /content/layout-model-training/ && pip install -r requirements.txt\n",
        "\n",
        "# Change working directory\n",
        "%cd /content/layout-model-training/\n",
        "\n",
        "# Clone forked detectron2 repo from FAIR\n",
        "! git clone https://github.com/matthewleechen/detectron2\n",
        "\n",
        "# Clone forked cocosplit repo from akarazniewicz\n",
        "! git clone https://github.com/matthewleechen/cocosplit\n",
        "! pip install -r cocosplit/requirements.txt\n",
        "\n",
        "# Install remaining dependencies\n",
        "! pip install -e git+https://github.com/matthewleechen/layout-parser.git#egg=layoutparser\n",
        "! pip install torchvision && pip install \"detectron2@git+https://github.com/facebookresearch/detectron2.git@v0.5#egg=detectron2\"\n",
        "! pip install google-cloud-vision"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Restart runtime before proceeding."
      ],
      "metadata": {
        "id": "EzAEbI3FOXng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Change working directory\n",
        "%cd /content/layout-model-training/"
      ],
      "metadata": {
        "id": "fXpMkx8csczi",
        "outputId": "423a06cc-6cc3-4ce7-cef6-c2fde2545b4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/layout-model-training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "import layoutparser as lp\n",
        "import cv2\n",
        "from concurrent.futures import ThreadPoolExecutor"
      ],
      "metadata": {
        "id": "Olpd6eawLma2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload the COCO annotations file to the current directory `/content/layout-model-training/`."
      ],
      "metadata": {
        "id": "Lwn5yMr8XzEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "daRQXNS5-Nik"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_file = \"patent_data.zip\"\n",
        "\n",
        "# Create data folder\n",
        "output_folder = \"data\"\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# Extract the contents of the annotations file to the data folder\n",
        "with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "    for member in zip_ref.namelist():\n",
        "        if not member.startswith('._'):\n",
        "            zip_ref.extract(member, output_folder)\n"
      ],
      "metadata": {
        "id": "yKrh0OxiToNJ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create outputs folder\n",
        "! mkdir outputs"
      ],
      "metadata": {
        "id": "ONDbHS4n4ndp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The outputs folder will contain evaluation data, checkpoint information and model weights following training."
      ],
      "metadata": {
        "id": "l61johhOUe53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split the data into training and test sets**"
      ],
      "metadata": {
        "id": "XFApjPaNva4T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code below allocates 80% of the data to the training set, and 20% to the test set. You can change this via the parameter `--s` currently set to 0.8."
      ],
      "metadata": {
        "id": "XffI2lydTv80"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data\n",
        "# Run the coco-split\n",
        "! python cocosplit/cocosplit.py --having-annotations --multi-class -s 0.8 data/result.json data/train.json data/test.json --seed 42"
      ],
      "metadata": {
        "id": "iMlVNVmUuw7K",
        "outputId": "f8b46690-a859-492a-fe9b-fdd8da304f93",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved 9016 entries in data/train.json and 2254 in data/test.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training Detectron2 vision models**"
      ],
      "metadata": {
        "id": "aJh1-_b8vgo-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Continue training from last checkpoint***\n",
        "\n",
        "Upload the `last_checkpoint` file and the model weights file (`model_{number of iterations}.pth`) to the outputs folder.\n",
        "\n",
        "***Start training from default pre-trained model weights***\n",
        "\n",
        "Ensure the outputs folder is empty.\n",
        "\n",
        "***Evaluation only***\n",
        "\n",
        "Pass the `--eval-only MODEL.WEIGHTS outputs/last_checkpoint` argument to the `train_annotations.sh` file.\n"
      ],
      "metadata": {
        "id": "wki93w1F6Oyl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The default model in `train_annotations.sh` is Faster-RCNN with a ResNet-50 backbone and a feature pyramid network (config file: `layout-model-training/configs/fast_rcnn_R_50_FPN_3x.yaml`). There is also Mask-RCNN with the same backbone and feature pyramid network (config file: `mask_rcnn_R_50_FPN_3x.yaml`). Mask-RCNN is an instance segmentation model built upon Faster-RCNN (so disabling segmentation masks leaves you with a Faster-RCNN model) and so you will need a COCO dataset with segmentation masks, or else an attribute error will be returned. You can try other models from the Detectron2 library (`layout-model-training/detectron2/configs`) by changing `cxonfig-file` in `train_annotations.sh`.\n",
        "\n",
        "Hyperparameters can be adjusted from the configuration files (some can be adjusted from `train_annotations.sh`). If training diverges, you will likely need to reduce the base learning rate (`BASE_LR` in the config file). Note that I used the hyperparameters from the config files in the cloned repository, which correspond to the default Detectron2 hyperparameters (except the base learning rate for Mask-RCNN which was halved to 0.01 because training diverged with a base learning rate of 0.02). I set the maximum iterations to 100,000 (from the default 60,000) and train all models to this iteration.\n",
        "\n",
        "I train Fast-RCNN using a subset of the annotations (only those from 1853). I train both Faster-RCNN and Mask-RCNN using the full set of annotations.\n",
        "\n",
        "Note that setting the seed on Detectron2 models does not guarantee deterministic behavior - see https://detectron2.readthedocs.io/en/latest/modules/config.html for further information."
      ],
      "metadata": {
        "id": "sassKe-lW5Px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "! bash scripts/train_annotations.sh"
      ],
      "metadata": {
        "id": "eThyeWoS1Tg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Visualize bounding box predictions (Optional)**"
      ],
      "metadata": {
        "id": "rEQKvnAEDjym"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = cv2.imread(\"/path/to/image\") # Set path to image you want to visualize\n",
        "layout = model.detect(image)"
      ],
      "metadata": {
        "id": "IWC7bn0uDk2h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "blocks = lp.Layout([b for b in layout if b.type=='text' or b.type=='date_box' or b.type=='full_box'])"
      ],
      "metadata": {
        "id": "uKhnZ8quDnE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_blocks = lp.Layout([b for b in layout if b.type=='text'])\n",
        "date_blocks = lp.Layout([b for b in layout if b.type=='date_box'])\n",
        "header_blocks = lp.Layout([b for b in layout if b.type=='header'])\n",
        "full_blocks = lp.Layout([b for b in layout if b.type=='full_blocks'])"
      ],
      "metadata": {
        "id": "smj1TZ7qDn_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization (box_width is the relative width of bounding box boundaries)\n",
        "lp.draw_box(image, blocks, box_width=10) # Can replace blocks with text_blocks, date_blocks or header_blocks"
      ],
      "metadata": {
        "id": "oFPXIFM8Do9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference**"
      ],
      "metadata": {
        "id": "5Z9sDMzkC-1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = lp.Detectron2LayoutModel(\n",
        "    config_path = \"/path/to/config/file\", # config file will be in outputs folder\n",
        "    model_path = \"/path/to/model/weights/file\", # model weights file will be in outputs folder\n",
        "    extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.5], # set confidence threshold here (replace 0.5 if desired)\n",
        "    label_map={0: \"date_box\", 1: \"full_box\", 2: \"header\", 3: \"text\"} # set label_map according to COCO dataset\n",
        ")"
      ],
      "metadata": {
        "id": "qRU6XYAiC_96"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload your Google Cloud Vision (GCV) credentials file to the current directory. You will need a Google account to use the GCV API. Information on getting started by setting up your credentials is available [here](https://developers.google.com/workspace/guides/create-credentials)."
      ],
      "metadata": {
        "id": "dMwqoPewDTj7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize GCV API\n",
        "ocr_agent = lp.GCVAgent.with_credential(\"/path/to/credentials\",\n",
        "                                        languages = ['en'])"
      ],
      "metadata": {
        "id": "YdIJP9z7DR9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original patent documents span several books. Before running inference, it is recommended to structure the unlabelled documents in separate directories by book. For example:\n",
        "\n",
        "```\n",
        "patent_data_woodcroft/\n",
        "└── chron_1617-1852-vol1/\n",
        "    ├── chron_1617-1852-vol1_0019.jp2\n",
        "    ├── chron_1617-1852-vol1_0020.jp2\n",
        "    ├── chron_1617-1852-vol1_0021.jp2\n",
        "    ...\n",
        "    ├── chron_1617-1852-vol1_0802.jp2\n",
        "└── chron_1617-1852-vol2/\n",
        "└── chron_1852-oct-dec/\n",
        "└── chron_1853/\n",
        "└── chron_1854/\n",
        "└── chron_1855/\n",
        "...\n",
        "└── chron_1871/\n",
        "```\n",
        "\n",
        "Each subdirectory of \"patent_data_woodcroft\" should contain all the image files you want to run inference on. Then, run the following three code cells for each directory (book) to obtain a merged text file for every book."
      ],
      "metadata": {
        "id": "SwO9RokZDZ1o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set folder path\n",
        "folder_path = \"/path/to/directory\""
      ],
      "metadata": {
        "id": "yrMrj7I1DaMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The inference loop below performs layout detection and OCR (using GCV) on the images in the folder path, and writes the output to a .txt file that inherits the same name as the original image file, separating each bounding box of text with a separator (---).\n",
        "\n",
        "The loop only runs over all image files within a folder (e.g. within `chron_1854`), and not over all folders. The reason is to allow for cost monitoring given that GCV charges on a per page basis. If cost monitoring is not a concern, you can modify the code block below to loop over all the folders.\n",
        "\n",
        "In order to run inference on all of the unlabelled original patent documents, this requires approximately 10-12 hours in total on a Colab CPU. Batch processing is limited on Colab by the small number of CPU cores. Inference will be completed much quicker if run locally on a multi-core unit or on a GPU."
      ],
      "metadata": {
        "id": "sIdppbQbDc3w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over documents and run inference\n",
        "def process_image(filepath):\n",
        "    # Construct the input and output file paths\n",
        "    output_filepath = os.path.splitext(filepath)[0] + '.txt'\n",
        "\n",
        "    # Perform layout detection and OCR on the image\n",
        "    image = cv2.imread(filepath)\n",
        "    layout = model.detect(image)\n",
        "    blocks = lp.Layout([b for b in layout if b.type=='text' or b.type=='date_box' or b.type=='full_box'])\n",
        "\n",
        "    with open(output_filepath, 'w') as f:\n",
        "        sorted_blocks = sorted(blocks, key=lambda b: b.coordinates[1]) # order by y-axis\n",
        "\n",
        "        for block in sorted_blocks: # padding\n",
        "            segment_image = (block\n",
        "                                .pad(left=5, right=5, top=5, bottom=5)\n",
        "                                .crop_image(image))\n",
        "\n",
        "            layout = ocr_agent.detect(segment_image)\n",
        "\n",
        "            full_text = ''\n",
        "            for line in layout:\n",
        "                text = line.text\n",
        "                if text.endswith('.'):\n",
        "                    full_text += text + '\\n'\n",
        "                else:\n",
        "                    # remove spaces before commas\n",
        "                    text = text.replace(' ,', ',')\n",
        "                    full_text += text + ' '\n",
        "\n",
        "            # remove space before full stops\n",
        "            full_text = full_text.replace(' .', '.')\n",
        "\n",
        "            # Write the output to the file\n",
        "            f.write(full_text.strip() + \"\\n\")\n",
        "            f.write('---\\n')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Get a list of all the JPEG files in the folder\n",
        "    filenames = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.lower().endswith('.jpg')]\n",
        "\n",
        "    # Batch process the images using multiple threads\n",
        "    with ThreadPoolExecutor(max_workers=2) as executor: # set max_workers = #cpu cores (2 on Colab)\n",
        "        executor.map(process_image, filenames)"
      ],
      "metadata": {
        "id": "wxXvxYqHDej0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code merges the saved text files across all pages in order of page number. The final output will be a merged text file containing all the patents in the relevant book (e.g. a single `merged.txt` file for every subdirectory of `patent_data_woodcroft`). A separator (---) will be between any two bounding boxes."
      ],
      "metadata": {
        "id": "wyd5eJEjDuc1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create merged text file from all individual pages.\n",
        "\n",
        "output_file = \"merged.txt\"\n",
        "\n",
        "# Get a list of all the .txt files in the directory, sorted by name\n",
        "files = [f for f in os.listdir(folder_path) if f.endswith(\".txt\")]\n",
        "files.sort()\n",
        "\n",
        "with open(os.path.join(folder_path, output_file), \"w\") as outfile:\n",
        "    for filename in files:\n",
        "        with open(os.path.join(folder_path, filename), \"r\") as infile:\n",
        "            content = infile.read().strip()\n",
        "            if content:  # Check if content is not empty\n",
        "                if outfile.tell() != 0:  # Check if output file is not empty\n",
        "                    outfile.write(\"---\\n\")  # Add separator between files\n",
        "                outfile.write(content)\n",
        "\n",
        "# Remove double separators\n",
        "with open(os.path.join(folder_path, output_file), \"r+\") as f:\n",
        "    lines = f.readlines()\n",
        "    f.seek(0)\n",
        "    for i, line in enumerate(lines):\n",
        "        if line.strip() != \"---\" or i == 0 or lines[i-1].strip() != \"---\":\n",
        "            f.write(line)\n",
        "    f.truncate()"
      ],
      "metadata": {
        "id": "vvjFIrhMDrwA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}