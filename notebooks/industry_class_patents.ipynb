{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthewleechen/woodcroft_patents/blob/main/industry_class/industry_class_patents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLB3I4FKZ5Lr"
      },
      "source": [
        "This notebook is based on Niels Rogge's (extremely helpful!) notebook, \"Fine-tuning BERT (and friends) for multi-label text classification\", linked [here](https://github.com/matthewleechen/Transformers-Tutorials/blob/master/BERT/Fine_tuning_BERT_(and_friends)_for_multi_label_text_classification.ipynb).\n",
        "\n",
        "It is not recommended to run this notebook on the Colab free plan. This notebook's training loop was originally run using Colab Pro on 1 Nvidia Tesla V100 (16GB) GPU. You can also run this locally on a virtual machine or server, but carefully check for dependencies.\n",
        "\n",
        "This notebook uses [MacBERTh](https://huggingface.co/emanjavacas/MacBERTh), a BERT model pre-trained on historical English (c.1450-1950), to classify inventions into industry categories (original paper linked [here](https://jdmdh.episciences.org/9690)). \n",
        "\n",
        "This notebook allows for any model available using [HuggingFace Transformers](https://huggingface.co/docs/transformers/index) to be used. I have experimented with BERT (base and both [cased](https://huggingface.co/bert-base-uncased)/[uncased](https://huggingface.co/bert-base-cased)), RoBERTa ([base](https://huggingface.co/roberta-base) and [distilled](https://huggingface.co/distilroberta-base)), XLNet ([base](https://huggingface.co/xlnet-base-cased)), and [SBERT](https://www.sbert.net) models and find that MacBERTh marginally outperforms across a range of hyperparameters."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup**"
      ],
      "metadata": {
        "id": "D74SxYjCDe0s"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wxY3x-ZZz8h"
      },
      "source": [
        "%%capture\n",
        "!pip install transformers==4.29.0 \n",
        "!pip install datasets\n",
        "!pip install accelerate"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "from transformers import set_seed, Trainer, AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, EvalPrediction, pipeline\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from sklearn.metrics import f1_score, roc_auc_score, accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "nYlb0IrmbGrv"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "-i-Jv2milbH0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIH9NP0MZ6-O"
      },
      "source": [
        "**Load data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sd1LiXGjZ420"
      },
      "source": [
        "# Load excel file into a pandas DataFrame\n",
        "df = pd.read_excel('labelled_data_patents.xlsx')\n",
        "\n",
        "# Encode the \"Industry\" column into separate columns using one-hot encoding\n",
        "df_encoded = pd.get_dummies(df['Industry'])\n",
        "\n",
        "# Merge the original DataFrame with the encoded columns\n",
        "df_final = pd.concat([df, df_encoded], axis=1)\n",
        "\n",
        "# Iterate over the columns and update the values to \"True\" or \"False\" based on the correct class\n",
        "for industry in df['Industry'].unique():\n",
        "    df_final[industry] = df_final['Industry'] == industry\n",
        "\n",
        "# Remove the original \"Industry\" column\n",
        "df_final.drop('Industry', axis=1, inplace=True)\n",
        "\n",
        "df_final.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset and train-test split\n",
        "dataset = df_final\n",
        "\n",
        "# Split the dataset into features and labels\n",
        "X = dataset[['num', 'text']]\n",
        "y = dataset.drop(['num', 'text'], axis=1)\n",
        "\n",
        "# Split the dataset into train, validation, and test sets\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, random_state=42)  # 0.25 * 0.8 = 0.2\n",
        "\n",
        "train_df = pd.concat([X_train, y_train], axis=1).reset_index(drop=True)\n",
        "val_df = pd.concat([X_val, y_val], axis=1).reset_index(drop=True)\n",
        "test_df = pd.concat([X_test, y_test], axis=1).reset_index(drop=True)\n",
        "\n",
        "# Drop the \"__index_level_0__\" column if it exists\n",
        "if '__index_level_0__' in train_df.columns:\n",
        "    train_df.drop('__index_level_0__', axis=1, inplace=True)\n",
        "\n",
        "if '__index_level_0__' in val_df.columns:\n",
        "    val_df.drop('__index_level_0__', axis=1, inplace=True)\n",
        "\n",
        "if '__index_level_0__' in test_df.columns:\n",
        "    test_df.drop('__index_level_0__', axis=1, inplace=True)\n",
        "    \n",
        "dataset = DatasetDict({\n",
        "    'train': Dataset.from_pandas(train_df),\n",
        "    'validation': Dataset.from_pandas(val_df),\n",
        "    'test': Dataset.from_pandas(test_df)\n",
        "})\n",
        "\n",
        "dataset\n"
      ],
      "metadata": {
        "id": "mDF95UknbJOM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRd1kXQZjYIY"
      },
      "source": [
        "# Visualize dataset as dictionary with 3 splits\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unjuTtKUjZI3"
      },
      "source": [
        "# check example entry\n",
        "example = dataset['train'][0]\n",
        "example"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e5vZhQpvkE8s"
      },
      "source": [
        "# Create labels\n",
        "labels = [label for label in dataset['train'].features.keys() if label not in ['num', 'text']]\n",
        "id2label = {idx:label for idx, label in enumerate(labels)}\n",
        "label2id = {label:idx for idx, label in enumerate(labels)}\n",
        "labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ3Teyjmank2"
      },
      "source": [
        "**Data Pre-processing**\n",
        "\n",
        "As models like BERT don't expect text as direct input, but rather `input_ids`, etc., we tokenize the text using the tokenizer. Here I'm using the `AutoTokenizer` API, which will automatically load the appropriate tokenizer based on the checkpoint on the hub.\n",
        "\n",
        "What's a bit tricky is that we also need to provide labels to the model. For multi-label text classification, this is a matrix of shape (batch_size, num_labels). Also important: this should be a tensor of floats rather than integers, otherwise PyTorch' `BCEWithLogitsLoss` (which the model will use) will complain, as explained [here](https://discuss.pytorch.org/t/multi-label-binary-classification-result-type-float-cant-be-cast-to-the-desired-output-type-long/117915/3)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFWlSsbZaRLc"
      },
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"emanjavacas/MacBERTh\") # change tokenizer here\n",
        "\n",
        "def preprocess_data(examples):\n",
        "  # take a batch of texts\n",
        "  text = examples[\"text\"]\n",
        "  # encode them\n",
        "  encoding = tokenizer(text, padding=\"max_length\", truncation=True, max_length=128)\n",
        "  # add labels\n",
        "  labels_batch = {k: examples[k] for k in examples.keys() if k in labels}\n",
        "  # create numpy array of shape (batch_size, num_labels)\n",
        "  labels_matrix = np.zeros((len(text), len(labels)))\n",
        "  # fill numpy array\n",
        "  for idx, label in enumerate(labels):\n",
        "    labels_matrix[:, idx] = labels_batch[label]\n",
        "\n",
        "  encoding[\"labels\"] = labels_matrix.tolist()\n",
        "  \n",
        "  return encoding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4ENBTdulBEI"
      },
      "source": [
        "encoded_dataset = dataset.map(preprocess_data, batched=True, remove_columns=dataset['train'].column_names)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0enAb0W9o25W"
      },
      "source": [
        "example = encoded_dataset['train'][0]\n",
        "print(example.keys())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D0McCtJ8HRJY"
      },
      "source": [
        "tokenizer.decode(example['input_ids'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VdIvj6WjHeZQ"
      },
      "source": [
        "example['labels']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q4Dx95t2o6N9"
      },
      "source": [
        "[id2label[idx] for idx, label in enumerate(example['labels']) if label == 1.0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lk6Cq9duKBkA"
      },
      "source": [
        "# Set PyTorch tensors\n",
        "encoded_dataset.set_format(\"torch\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w5qSmCgWefWs"
      },
      "source": [
        "**Define model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6XPL1Z_RegBF"
      },
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\"emanjavacas/MacBERTh\", # change model here\n",
        "                                                           problem_type=\"multi_label_classification\", \n",
        "                                                           num_labels=len(labels),\n",
        "                                                           id2label=id2label,\n",
        "                                                           label2id=label2id)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjJGEXShp7te"
      },
      "source": [
        "**Training** \n",
        "\n",
        "Training uses HuggingFace's [Trainer API](https://huggingface.co/docs/transformers/main_classes/trainer): hyperparameters are specified using [TrainingArguments](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments) and the training loop is specified using the [Trainer](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.Trainer) object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5a8_vIKqr7P"
      },
      "source": [
        "batch_size = 16\n",
        "metric_name = \"f1\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dR2GmpvDqbuZ"
      },
      "source": [
        "# Specify hyperparameters\n",
        "args = TrainingArguments(\n",
        "    f\"emanjavacas/MacBERTh\", # change model here\n",
        "    evaluation_strategy = \"epoch\",\n",
        "    save_strategy = \"epoch\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=5,\n",
        "    weight_decay=0.01,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=metric_name\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "797b2WHJqUgZ"
      },
      "source": [
        "# define function to compute metrics while training\n",
        "# source: https://jesusleal.io/2021/04/21/Longformer-multilabel-classification/\n",
        "def multi_label_metrics(predictions, labels, threshold=0.5):\n",
        "    # first, apply sigmoid on predictions which are of shape (batch_size, num_labels)\n",
        "    sigmoid = torch.nn.Sigmoid()\n",
        "    probs = sigmoid(torch.Tensor(predictions))\n",
        "    # next, use threshold to turn them into integer predictions\n",
        "    y_pred = np.zeros(probs.shape)\n",
        "    y_pred[np.where(probs >= threshold)] = 1\n",
        "    # finally, compute metrics\n",
        "    y_true = labels\n",
        "    f1_micro_average = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
        "    roc_auc = roc_auc_score(y_true, y_pred, average = 'micro')\n",
        "    accuracy = accuracy_score(y_true, y_pred)\n",
        "    # return as dictionary\n",
        "    metrics = {'f1': f1_micro_average,\n",
        "               'roc_auc': roc_auc,\n",
        "               'accuracy': accuracy}\n",
        "    return metrics\n",
        "\n",
        "def compute_metrics(p: EvalPrediction):\n",
        "    preds = p.predictions[0] if isinstance(p.predictions, \n",
        "            tuple) else p.predictions\n",
        "    result = multi_label_metrics(\n",
        "        predictions=preds, \n",
        "        labels=p.label_ids)\n",
        "    return result"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlOgGiojuWwG"
      },
      "source": [
        "encoded_dataset['train'][0]['labels'].type()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y41Kre_jvD7x"
      },
      "source": [
        "encoded_dataset['train']['input_ids'][0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxWcnZ8ku12V"
      },
      "source": [
        "#forward pass\n",
        "outputs = model(input_ids=encoded_dataset['train']['input_ids'][0].unsqueeze(0), labels=encoded_dataset['train'][0]['labels'].unsqueeze(0))\n",
        "outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chq_3nUz73ib"
      },
      "source": [
        "# specify training loop\n",
        "trainer = Trainer(\n",
        "    model,\n",
        "    args,\n",
        "    train_dataset=encoded_dataset[\"train\"],\n",
        "    eval_dataset=encoded_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KXmFds8js6P8"
      },
      "source": [
        "# Run training\n",
        "trainer.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiloh9eMK91o"
      },
      "source": [
        "**Evaluation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cMlebJ83LRYG"
      },
      "source": [
        "# Run evaluation\n",
        "trainer.evaluate()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "model_path = \"/content/emanjavacas\"\n",
        "\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ],
      "metadata": {
        "id": "B3SZSTjT8zTI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference**"
      ],
      "metadata": {
        "id": "64epaKGKAEn8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code below runs inference using the HuggingFace Pipelines API - documentation is linked [here](https://huggingface.co/docs/transformers/main_classes/pipelines).\n",
        "\n",
        "Running this code on a GPU is strongly recommended. A Nvidia Tesla T4 GPU (provided on the Colab free plan) is orders of magnitude faster than using the CPU. On the T4, inference on the full set of patents takes approximately 20-25 minutes, but several hours on the Colab CPU."
      ],
      "metadata": {
        "id": "yoF-i97rRKwH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Deploy model and tokenizer for inference\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "wqaAgmIhRBnv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deploy Pipeline API: device = 0 for GPU, device = -1 is default (for CPU)\n",
        "pipe = pipeline(task=\"text-classification\", model=model, device = 0, tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "1G_5hKCiUOhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define input and output files\n",
        "input_file = \"/path/to/input/file\" # path to cleaned ner output\n",
        "output_file = \"/path/to/output/file\" # path to outputted file"
      ],
      "metadata": {
        "id": "Rv6FfqazZD2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load in cleaned NER output\n",
        "df = pd.read_excel(input_file)\n",
        "\n",
        "# Run inference loop\n",
        "def classify(phrase):\n",
        "    result = pipe(phrase)\n",
        "    return result[0][\"label\"]\n",
        "\n",
        "# Apply the function on the misc column and save the output to pred_industry column\n",
        "df[\"misc\"] = df[\"misc\"].astype(str)\n",
        "df[\"pred_industry\"] = \"\"\n",
        "\n",
        "with tqdm(total=len(df), desc=\"Classifying\") as pbar:\n",
        "    for index, row in df.iterrows():\n",
        "        df.loc[index, \"pred_industry\"] = classify(row[\"misc\"])\n",
        "        pbar.update(1)\n",
        "\n",
        "# Save the updated dataframe to a new excel file\n",
        "df.to_excel(output_file, index=False)"
      ],
      "metadata": {
        "id": "aAHY4CeoW9Kl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
