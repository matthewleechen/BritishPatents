{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/matthewleechen/woodcroft_patents/blob/main/ner/notebooks/inference.ipynb",
      "authorship_tag": "ABX9TyPODVC0yg3fc8Q8JV6dwPxR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthewleechen/woodcroft_patents/blob/main/ner/notebooks/inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is designed to be run on Google Colab. You can run it locally but you will need to check dependencies carefully. \n",
        "\n",
        "This notebook is designed for running inference using the fine-tuned weights from any model in your directory containing the model weights and configuration files. "
      ],
      "metadata": {
        "id": "4Xfadjue1bpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "LypF9HWf3gcp"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForTokenClassification, AutoTokenizer, pipeline\n",
        "import numpy as np\n",
        "import csv\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "pzaj9Ih11d9J"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will need to upload the output files generated by using the save_pretrained method after fine tuning to a directory if they are not in Google Drive."
      ],
      "metadata": {
        "id": "2Of1r1u62Z6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model weights and configuration\n",
        "model_path = \"/path/to/model/weights\"\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "bD4aBiSX2XaZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code below runs inference on the merged text boxes (each is referred to as a \"sentence\") using the HuggingFace Pipelines API - documentation is linked [here](https://huggingface.co/docs/transformers/main_classes/pipelines).\n",
        "\n",
        "This assumes that you have an input directory consisting of .txt files, and have an output directory that you want .csv files to exported to. The output is a csv file containing the labelled classes as columns (with each entity outputted to a separate column for the classes \"PER\", \"LOC\" and \"OCC\"), and each patent being recorded as a row (observation). You may need to modify this code depending on your desired output format.\n",
        "\n",
        "Running this code on a cheap GPU is strongly recommended. A Nvidia Tesla T4 GPU (provided on the Colab free plan) is orders of magnitude faster than using the CPU. On the T4, inference on 1000 patents takes approximately 15-20 seconds, but several hours on the Colab CPU."
      ],
      "metadata": {
        "id": "3QoFUCsVQ_r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set input and output directories\n",
        "input_dir = \"/path/to/input/dir\"\n",
        "output_dir = \"/path/to/output/dir\""
      ],
      "metadata": {
        "id": "isuamE_9mcdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "QrijFKEYmc-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deploy Pipeline API: device = 0 for GPU, device = -1 is default (for CPU)\n",
        "pipe = pipeline(task=\"token-classification\", model=model, device = 0, tokenizer=tokenizer, aggregation_strategy=\"simple\")"
      ],
      "metadata": {
        "id": "OBpJ4-I_mgsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over all .txt files in input directory\n",
        "for filename in tqdm(os.listdir(input_dir)):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        # Specify file paths\n",
        "        input_path = os.path.join(input_dir, filename)\n",
        "        output_path = os.path.join(output_dir, filename[:-4] + \".csv\")  # Remove .txt extension and add .csv extension\n",
        "\n",
        "        # Read sentences from text file\n",
        "        with open(input_path, \"r\") as f:\n",
        "            sentences = f.read().split(\"\\n\\n\")\n",
        "\n",
        "        # Create list of dictionaries to store entities for each sentence\n",
        "        all_entities = []\n",
        "        fieldnames = [\"NUM\", \"PER\", \"DATE\", \"LOC\", \"COMM\", \"OCC\", \"MISC\", \"INFO\"]  # Specify the fieldnames\n",
        "\n",
        "        for sentence in sentences:\n",
        "            # Extract entities\n",
        "            combined_entities = {}\n",
        "            for entity in pipe(sentence):\n",
        "                entity_group = entity['entity_group']\n",
        "                word = entity['word']\n",
        "                if entity_group not in combined_entities:\n",
        "                    combined_entities[entity_group] = []\n",
        "                combined_entities[entity_group].append(word)\n",
        "\n",
        "            # Create a new dictionary to store the updated entities\n",
        "            updated_entities = {}\n",
        "            for entity_group, words in combined_entities.items():\n",
        "                if entity_group in [\"PER\"]:\n",
        "                    for i, word in enumerate(words):\n",
        "                        column_name = f\"{entity_group}_{i + 1}\"\n",
        "                        if column_name not in fieldnames:\n",
        "                            updated_entities[column_name] = word\n",
        "                else:\n",
        "                    updated_entities[entity_group] = '& '.join(words)\n",
        "\n",
        "            # Add updated entities for this sentence to list\n",
        "            all_entities.append(updated_entities)\n",
        "\n",
        "        # Update fieldnames to include separate columns for PER, LOC, and OCC\n",
        "        max_columns = {key: 0 for key in [\"PER\"]}\n",
        "        for entity_dict in all_entities:\n",
        "            for key in max_columns.keys():\n",
        "                max_columns[key] = max(max_columns[key], len([k for k in entity_dict.keys() if k.startswith(key)]))\n",
        "\n",
        "        for key, count in max_columns.items():\n",
        "            for i in range(count):\n",
        "                column_name = f\"{key}_{i + 1}\"\n",
        "                if column_name not in fieldnames:\n",
        "                    fieldnames.append(column_name)\n",
        "\n",
        "        # Write entities to CSV file\n",
        "        with open(output_path, 'w', newline='') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(all_entities)\n"
      ],
      "metadata": {
        "id": "lShktEYOllBR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}