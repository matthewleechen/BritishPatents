{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "https://github.com/matthewleechen/ner_patents/blob/main/inference.ipynb",
      "authorship_tag": "ABX9TyMA6z6H9oEpF/EEbMMu/XUl",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthewleechen/woodcroft_patents/blob/main/ner/notebooks/inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is designed to be run on Google Colab. You can run it locally but you will need to check dependencies carefully. "
      ],
      "metadata": {
        "id": "4Xfadjue1bpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "LypF9HWf3gcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import BertForTokenClassification, BertTokenizer, pipeline\n",
        "import numpy as np\n",
        "import csv\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "pzaj9Ih11d9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You will need to upload the output files generated by using the save_pretrained method after fine tuning to a directory if they are not in Google Drive."
      ],
      "metadata": {
        "id": "2Of1r1u62Z6y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model weights and configuration\n",
        "model_path = \"/path/to/saved/model\"\n",
        "model = BertForTokenClassification.from_pretrained(model_path)\n",
        "tokenizer = BertTokenizer.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "bD4aBiSX2XaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code below runs inference on the merged text boxes (each is referred to as a \"sentence\") using the HuggingFace Pipelines API - documentation is linked [here](https://huggingface.co/docs/transformers/main_classes/pipelines).\n",
        "\n",
        "This assumes that you have an input directory consisting of .txt files, and have an output directory that you want .csv files to exported to. The output is a csv file containing the labelled classes as columns (with each entity outputted to a separate column for the classes \"PER\", \"LOC\" and \"OCC\"), and each patent being recorded as a row (observation). You may need to modify this code depending on your desired output format.\n",
        "\n",
        "Running this code on a cheap GPU is strongly recommended. A Nvidia Tesla T4 GPU (provided on the Colab free plan) is orders of magnitude faster than using the CPU. On the T4, inference on 1000 patents takes approximately 15-20 seconds, but several hours on the Colab CPU."
      ],
      "metadata": {
        "id": "3QoFUCsVQ_r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Set input and output directories\n",
        "input_dir = \"/path/to/input/directory\"\n",
        "output_dir = \"/path/to/output/directory\""
      ],
      "metadata": {
        "id": "isuamE_9mcdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "QrijFKEYmc-v",
        "outputId": "7e09234e-d04f-4af6-d7b2-6a7edb721485",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForTokenClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=17, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Deploy Pipeline API: device = 0 for GPU, device = -1 is default (for CPU)\n",
        "pipe = pipeline(task=\"token-classification\", model=model, device = 0, tokenizer=tokenizer, aggregation_strategy=\"simple\")"
      ],
      "metadata": {
        "id": "OBpJ4-I_mgsB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over all .txt files in input directory\n",
        "for filename in tqdm(os.listdir(input_dir)):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        # Specify file paths\n",
        "        input_path = os.path.join(input_dir, filename)\n",
        "        output_path = os.path.join(output_dir, filename[:-4] + \".csv\")  # Remove .txt extension and add .csv extension\n",
        "\n",
        "        # Read sentences from text file\n",
        "        with open(input_path, \"r\") as f:\n",
        "            sentences = f.read().split(\"\\n\\n\")\n",
        "\n",
        "        # Create list of dictionaries to store entities for each sentence\n",
        "        all_entities = []\n",
        "        fieldnames = [\"NUM\", \"PER\", \"DATE\", \"LOC\", \"COMM\", \"OCC\", \"MISC\", \"INFO\"]  # Specify the fieldnames\n",
        "\n",
        "        for sentence in sentences:\n",
        "            # Extract entities\n",
        "            combined_entities = {}\n",
        "            for entity in pipe(sentence):\n",
        "                entity_group = entity['entity_group']\n",
        "                word = entity['word']\n",
        "                if entity_group not in combined_entities:\n",
        "                    combined_entities[entity_group] = []\n",
        "                combined_entities[entity_group].append(word)\n",
        "\n",
        "            # Create a new dictionary to store the updated entities\n",
        "            updated_entities = {}\n",
        "            for entity_group, words in combined_entities.items():\n",
        "                if entity_group in [\"PER\", \"LOC\", \"OCC\"]:\n",
        "                    for i, word in enumerate(words):\n",
        "                        column_name = f\"{entity_group}_{i + 1}\"\n",
        "                        if column_name not in fieldnames:\n",
        "                            updated_entities[column_name] = word\n",
        "                else:\n",
        "                    updated_entities[entity_group] = '& '.join(words)\n",
        "\n",
        "            # Add updated entities for this sentence to list\n",
        "            all_entities.append(updated_entities)\n",
        "\n",
        "        # Update fieldnames to include separate columns for PER, LOC, and OCC\n",
        "        max_columns = {key: 0 for key in [\"PER\", \"LOC\", \"OCC\"]}\n",
        "        for entity_dict in all_entities:\n",
        "            for key in max_columns.keys():\n",
        "                max_columns[key] = max(max_columns[key], len([k for k in entity_dict.keys() if k.startswith(key)]))\n",
        "\n",
        "        for key, count in max_columns.items():\n",
        "            for i in range(count):\n",
        "                column_name = f\"{key}_{i + 1}\"\n",
        "                if column_name not in fieldnames:\n",
        "                    fieldnames.append(column_name)\n",
        "\n",
        "        # Write entities to CSV file\n",
        "        with open(output_path, 'w', newline='') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(all_entities)\n"
      ],
      "metadata": {
        "id": "lShktEYOllBR",
        "outputId": "0c92984d-5e7c-43a4-f3d6-7cc5298fa928",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/23 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py:1080: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n",
            "100%|██████████| 23/23 [19:41<00:00, 51.35s/it]\n"
          ]
        }
      ]
    }
  ]
}