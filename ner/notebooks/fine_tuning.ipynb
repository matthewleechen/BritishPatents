{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthewleechen/woodcroft_patents/blob/main/ner/notebooks/fine_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is a slightly modified clone of Niels Rogge's (extremely helpful!) notebook, \"Fine-tuning BERT for named entity recognition\", linked [here](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT.ipynb). \n",
        "\n",
        "It is **not** recommended to run this notebook on the Colab free plan. This notebook's training loop was originally run using Colab Pro on 1 Nvidia Tesla V100 (16GB) GPU. You can also run this locally on a virtual machine or server, but carefully check for dependencies."
      ],
      "metadata": {
        "id": "691hIbZN-Fvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing**"
      ],
      "metadata": {
        "id": "1610Mty_oJ_u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers seqeval[gpu]\n",
        "!pip install conllu"
      ],
      "metadata": {
        "id": "OtR8-rnn8frR"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import RobertaTokenizer, RobertaConfig, RobertaForTokenClassification, set_seed\n",
        "import conllu\n",
        "import csv"
      ],
      "metadata": {
        "id": "_KagjDqfbPSQ"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "id": "fy7-Z_OTbWNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8431c83-f437-4238-deb9-5378baece06d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "Nz8_izzzUPky"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload .conll dataset exported from Label Studio"
      ],
      "metadata": {
        "id": "WVK__AbdXtJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize annotations as .conll dataset\n",
        "data = open(\"/path/to/conll/dataset\", mode = \"r\", encoding = \"utf-8\")\n",
        "annotations = data.read()\n",
        "print(annotations[:1000])"
      ],
      "metadata": {
        "id": "-NAddj2PU8K6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "19a2e07f-c1c2-4f73-e114-8888dd9898e5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-DOCSTART- -X- O\n",
            "JOLY. -X- _ O\n",
            "24th -X- _ B-DATE\n",
            "November -X- _ I-DATE\n",
            "1852. -X- _ I-DATE\n",
            "852. -X- _ B-NUM\n",
            "ALPHONSE -X- _ B-PER\n",
            "JOLY -X- _ I-PER\n",
            ", -X- _ O\n",
            "of -X- _ O\n",
            "Paris -X- _ B-LOC\n",
            ", -X- _ I-LOC\n",
            "in -X- _ I-LOC\n",
            "the -X- _ I-LOC\n",
            "Republic -X- _ I-LOC\n",
            "of -X- _ I-LOC\n",
            "France -X- _ I-LOC\n",
            ", -X- _ O\n",
            "Civil -X- _ B-OCC\n",
            "Engineer -X- _ I-OCC\n",
            ", -X- _ O\n",
            "for -X- _ O\n",
            "an -X- _ O\n",
            "invention -X- _ O\n",
            "for -X- _ O\n",
            "\" -X- _ O\n",
            "Certain -X- _ B-MISC\n",
            "improvements -X- _ I-MISC\n",
            "in -X- _ I-MISC\n",
            "steam -X- _ I-MISC\n",
            "engines. -X- _ I-MISC\n",
            "\" -X- _ O\n",
            "Provisional -X- _ B-INFO\n",
            "protection -X- _ I-INFO\n",
            "only -X- _ I-INFO\n",
            ". -X- _ I-INFO\n",
            "\n",
            "LENOX. -X- _ O\n",
            "ROBERTS -X- _ O\n",
            ", -X- _ O\n",
            "18th -X- _ B-DATE\n",
            "October -X- _ I-DATE\n",
            "1852. -X- _ I-DATE\n",
            "426. -X- _ B-NUM\n",
            "GEORGE -X- _ B-PER\n",
            "WILLIAM -X- _ I-PER\n",
            "LENOX -X- _ I-PER\n",
            ", -X- _ O\n",
            "of -X- _ O\n",
            "Billiter -X- _ B-LOC\n",
            "Square -X- _ I-LOC\n",
            ", -X- _ I-LOC\n",
            "in -X- _ I-LOC\n",
            "the -X- _ I-LOC\n",
            "City -X- _ I-LOC\n",
            "of -X- _ I-LOC\n",
            "London -X- _ I-LOC\n",
            ", -X- _ O\n",
            "Chain -X- _ B-OCC\n",
            "Cable -X- _ I-OCC\n",
            "Manufacturer -X- _ I-\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# define input and output file names\n",
        "input_file = \"ner_patents.conll\" ## replace \"ner_patents\" with name of conll file\n",
        "output_file = \"ner_patents.csv\" ## same here\n",
        "\n",
        "# initialize the csv writer and write the header row\n",
        "csv_writer = csv.writer(open(output_file, \"w\", newline=\"\", encoding=\"utf-8\"))\n",
        "csv_writer.writerow([\"sentence_no\", \"word\", \"tag\"])\n",
        "\n",
        "# read the input file line by line\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    sentence_num = 1\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            # empty line denotes end of sentence\n",
        "            sentence_num += 1\n",
        "        else:\n",
        "            # split line into columns\n",
        "            columns = line.split()\n",
        "            word = columns[0]\n",
        "            tag = columns[-1]\n",
        "            sentence = \"{}\".format(sentence_num)\n",
        "            # write row to csv file\n",
        "            csv_writer.writerow([sentence, word or \"NaN\", tag or \"NaN\"])"
      ],
      "metadata": {
        "id": "amhEpGo4fv5O"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize data\n",
        "pd.read_csv('ner_patents.csv').drop(0).to_csv('ner_patents.csv', index=False)\n",
        "data = pd.read_csv(\"ner_patents.csv\", encoding='unicode_escape')\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "0ROSgiRkj1q5",
        "outputId": "4f96d0e7-161e-4a1e-8d9e-e1a1e4287db5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sentence_no      word     tag\n",
              "0            1     JOLY.       O\n",
              "1            1      24th  B-DATE\n",
              "2            1  November  I-DATE\n",
              "3            1     1852.  I-DATE\n",
              "4            1      852.   B-NUM"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4f0429f0-f388-4476-bf18-0c26f3aa5122\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_no</th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>JOLY.</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>24th</td>\n",
              "      <td>B-DATE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>November</td>\n",
              "      <td>I-DATE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1852.</td>\n",
              "      <td>I-DATE</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>852.</td>\n",
              "      <td>B-NUM</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4f0429f0-f388-4476-bf18-0c26f3aa5122')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4f0429f0-f388-4476-bf18-0c26f3aa5122 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4f0429f0-f388-4476-bf18-0c26f3aa5122');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by sentence\n",
        "# let's create a new column called \"patent\" which groups the words by sentence \n",
        "data['sentence'] = data[['sentence_no','word','tag']].groupby(['sentence_no'])['word'].transform(lambda x: ' '.join(x))\n",
        "# let's also create a new column called \"word_labels\" which groups the tags by sentence \n",
        "data['word_labels'] = data[['sentence_no','word','tag']].groupby(['sentence_no'])['tag'].transform(lambda x: ','.join(x))\n",
        "# Show data\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "03Eb_UtokFPk",
        "outputId": "b10e2eb4-1578-4311-b1dd-4f34b973f17e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sentence_no      word     tag  \\\n",
              "0            1     JOLY.       O   \n",
              "1            1      24th  B-DATE   \n",
              "2            1  November  I-DATE   \n",
              "3            1     1852.  I-DATE   \n",
              "4            1      852.   B-NUM   \n",
              "\n",
              "                                            sentence  \\\n",
              "0  JOLY. 24th November 1852. 852. ALPHONSE JOLY ,...   \n",
              "1  JOLY. 24th November 1852. 852. ALPHONSE JOLY ,...   \n",
              "2  JOLY. 24th November 1852. 852. ALPHONSE JOLY ,...   \n",
              "3  JOLY. 24th November 1852. 852. ALPHONSE JOLY ,...   \n",
              "4  JOLY. 24th November 1852. 852. ALPHONSE JOLY ,...   \n",
              "\n",
              "                                         word_labels  \n",
              "0  O,B-DATE,I-DATE,I-DATE,B-NUM,B-PER,I-PER,O,O,B...  \n",
              "1  O,B-DATE,I-DATE,I-DATE,B-NUM,B-PER,I-PER,O,O,B...  \n",
              "2  O,B-DATE,I-DATE,I-DATE,B-NUM,B-PER,I-PER,O,O,B...  \n",
              "3  O,B-DATE,I-DATE,I-DATE,B-NUM,B-PER,I-PER,O,O,B...  \n",
              "4  O,B-DATE,I-DATE,I-DATE,B-NUM,B-PER,I-PER,O,O,B...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-61d550f2-75c0-4535-ab7b-698d725754ae\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_no</th>\n",
              "      <th>word</th>\n",
              "      <th>tag</th>\n",
              "      <th>sentence</th>\n",
              "      <th>word_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>JOLY.</td>\n",
              "      <td>O</td>\n",
              "      <td>JOLY. 24th November 1852. 852. ALPHONSE JOLY ,...</td>\n",
              "      <td>O,B-DATE,I-DATE,I-DATE,B-NUM,B-PER,I-PER,O,O,B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>24th</td>\n",
              "      <td>B-DATE</td>\n",
              "      <td>JOLY. 24th November 1852. 852. ALPHONSE JOLY ,...</td>\n",
              "      <td>O,B-DATE,I-DATE,I-DATE,B-NUM,B-PER,I-PER,O,O,B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>November</td>\n",
              "      <td>I-DATE</td>\n",
              "      <td>JOLY. 24th November 1852. 852. ALPHONSE JOLY ,...</td>\n",
              "      <td>O,B-DATE,I-DATE,I-DATE,B-NUM,B-PER,I-PER,O,O,B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>1852.</td>\n",
              "      <td>I-DATE</td>\n",
              "      <td>JOLY. 24th November 1852. 852. ALPHONSE JOLY ,...</td>\n",
              "      <td>O,B-DATE,I-DATE,I-DATE,B-NUM,B-PER,I-PER,O,O,B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>852.</td>\n",
              "      <td>B-NUM</td>\n",
              "      <td>JOLY. 24th November 1852. 852. ALPHONSE JOLY ,...</td>\n",
              "      <td>O,B-DATE,I-DATE,I-DATE,B-NUM,B-PER,I-PER,O,O,B...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-61d550f2-75c0-4535-ab7b-698d725754ae')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-61d550f2-75c0-4535-ab7b-698d725754ae button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-61d550f2-75c0-4535-ab7b-698d725754ae');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make dictionary mapping tags to indices\n",
        "label2id = {k: v for v, k in enumerate(data.tag.unique())}\n",
        "id2label = {v: k for v, k in enumerate(data.tag.unique())}\n",
        "label2id"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8yTdhHcendoC",
        "outputId": "68b35853-591c-4931-e126-b34b9ec7639b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'O': 0,\n",
              " 'B-DATE': 1,\n",
              " 'I-DATE': 2,\n",
              " 'B-NUM': 3,\n",
              " 'B-PER': 4,\n",
              " 'I-PER': 5,\n",
              " 'B-LOC': 6,\n",
              " 'I-LOC': 7,\n",
              " 'B-OCC': 8,\n",
              " 'I-OCC': 9,\n",
              " 'B-MISC': 10,\n",
              " 'I-MISC': 11,\n",
              " 'B-INFO': 12,\n",
              " 'I-INFO': 13,\n",
              " 'B-COMM': 14,\n",
              " 'I-COMM': 15,\n",
              " 'I-NUM': 16}"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n",
        "data.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "-FsYm5F6nzYT",
        "outputId": "4e3189b6-dabf-447c-b6d9-dfb0d32c36dd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            sentence  \\\n",
              "0  JOLY. 24th November 1852. 852. ALPHONSE JOLY ,...   \n",
              "1  LENOX. ROBERTS , 18th October 1852. 426. GEORG...   \n",
              "2  EILER. 1st October 1852. 75. LAURENTIUS MATHIA...   \n",
              "3  943. HENRY HITCHINS , of King William Street ,...   \n",
              "4  GREAVES. 7th October 1852. 283. THOMAS GREAVES...   \n",
              "\n",
              "                                         word_labels  \n",
              "0  O,B-DATE,I-DATE,I-DATE,B-NUM,B-PER,I-PER,O,O,B...  \n",
              "1  O,O,O,B-DATE,I-DATE,I-DATE,B-NUM,B-PER,I-PER,I...  \n",
              "2  O,B-DATE,I-DATE,I-DATE,B-NUM,B-PER,I-PER,I-PER...  \n",
              "3  B-NUM,B-PER,I-PER,O,O,B-LOC,I-LOC,I-LOC,I-LOC,...  \n",
              "4  O,B-DATE,I-DATE,I-DATE,B-NUM,B-PER,I-PER,O,O,B...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-3faca7b8-80a6-4e35-96e8-87b461984e2f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>word_labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>JOLY. 24th November 1852. 852. ALPHONSE JOLY ,...</td>\n",
              "      <td>O,B-DATE,I-DATE,I-DATE,B-NUM,B-PER,I-PER,O,O,B...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>LENOX. ROBERTS , 18th October 1852. 426. GEORG...</td>\n",
              "      <td>O,O,O,B-DATE,I-DATE,I-DATE,B-NUM,B-PER,I-PER,I...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>EILER. 1st October 1852. 75. LAURENTIUS MATHIA...</td>\n",
              "      <td>O,B-DATE,I-DATE,I-DATE,B-NUM,B-PER,I-PER,I-PER...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>943. HENRY HITCHINS , of King William Street ,...</td>\n",
              "      <td>B-NUM,B-PER,I-PER,O,O,B-LOC,I-LOC,I-LOC,I-LOC,...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>GREAVES. 7th October 1852. 283. THOMAS GREAVES...</td>\n",
              "      <td>O,B-DATE,I-DATE,I-DATE,B-NUM,B-PER,I-PER,O,O,B...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-3faca7b8-80a6-4e35-96e8-87b461984e2f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-3faca7b8-80a6-4e35-96e8-87b461984e2f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-3faca7b8-80a6-4e35-96e8-87b461984e2f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset and Dataloader**"
      ],
      "metadata": {
        "id": "rQScj-PQoTQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define variables\n",
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 152\n",
        "VALID_BATCH_SIZE = 64\n",
        "EPOCHS = 150\n",
        "LEARNING_RATE = 1e-05\n",
        "MAX_GRAD_NORM = 10\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')"
      ],
      "metadata": {
        "id": "J6AjggpdoWN3"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word-level tokenization\n",
        "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
        "    \"\"\"\n",
        "    Word piece tokenization makes it difficult to match word labels\n",
        "    back up with individual word pieces. This function tokenizes each\n",
        "    word one at a time so that it is easier to preserve the correct\n",
        "    label for each subword. It is, of course, a bit slower in processing\n",
        "    time, but it will help our model achieve higher accuracy.\n",
        "    \"\"\"\n",
        "\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
        "\n",
        "        # Tokenize the word and count # of subwords the word is broken into\n",
        "        tokenized_word = tokenizer.tokenize(word)\n",
        "        n_subwords = len(tokenized_word)\n",
        "\n",
        "        # Add the tokenized word to the final tokenized word list\n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "        # Add the same label to the new list of labels `n_subwords` times\n",
        "        labels.extend([label] * n_subwords)\n",
        "\n",
        "    return tokenized_sentence, labels"
      ],
      "metadata": {
        "id": "IYO8qiEMoYxx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # step 1: tokenize (and adapt corresponding labels)\n",
        "        sentence = self.data.sentence[index]  \n",
        "        word_labels = self.data.word_labels[index]  \n",
        "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
        "        \n",
        "        # step 2: add special tokens (and corresponding labels)\n",
        "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
        "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
        "        labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
        "\n",
        "        # step 3: truncating/padding\n",
        "        maxlen = self.max_len\n",
        "\n",
        "        if (len(tokenized_sentence) > maxlen):\n",
        "          # truncate\n",
        "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
        "          labels = labels[:maxlen]\n",
        "        else:\n",
        "          # pad\n",
        "          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
        "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
        "\n",
        "        # step 4: obtain the attention mask\n",
        "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
        "        \n",
        "        # step 5: convert tokens to input ids\n",
        "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
        "\n",
        "        label_ids = [label2id[label] for label in labels]\n",
        "        # the following line is deprecated\n",
        "        #label_ids = [label if label != 0 else -100 for label in label_ids]\n",
        "        \n",
        "        return {\n",
        "              'ids': torch.tensor(ids, dtype=torch.long),\n",
        "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
        "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
        "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
        "        } \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "vAi5L8sao2R1"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 0.8\n",
        "train_dataset = data.sample(frac=train_size,random_state=200)\n",
        "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(data.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urVPkbLLphwT",
        "outputId": "abf7ab0d-3ced-4e20-987b-9c84516818c8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FULL Dataset: (596, 2)\n",
            "TRAIN Dataset: (477, 2)\n",
            "TEST Dataset: (119, 2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"ids\"][:30]), training_set[0][\"targets\"][:30]):\n",
        "  print('{0:10}  {1}'.format(token, id2label[label.item()]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tIPNdnbLeHot",
        "outputId": "276c5f66-aed4-456e-8c15-ee208f034092"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<unk>       O\n",
            "14          B-NUM\n",
            ",           B-NUM\n",
            "09          B-NUM\n",
            "1           B-NUM\n",
            ".           B-NUM\n",
            "A           O\n",
            "gr          O\n",
            "ant         O\n",
            "un          O\n",
            "to          O\n",
            "AL          B-PER\n",
            "FR          B-PER\n",
            "ED          B-PER\n",
            "T           I-PER\n",
            "AY          I-PER\n",
            "L           I-PER\n",
            "OR          I-PER\n",
            ",           O\n",
            "of          O\n",
            "War         B-LOC\n",
            "wick        B-LOC\n",
            "L           I-LOC\n",
            "ane         I-LOC\n",
            ",           I-LOC\n",
            "in          I-LOC\n",
            "the         I-LOC\n",
            "city        I-LOC\n",
            "of          I-LOC\n",
            "London      I-LOC\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)\n"
      ],
      "metadata": {
        "id": "CFkAHnqapp7A"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the model**"
      ],
      "metadata": {
        "id": "T-qAr1OjpsTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = RobertaForTokenClassification.from_pretrained('roberta-base', \n",
        "                                                   num_labels=len(id2label),\n",
        "                                                   id2label=id2label,\n",
        "                                                   label2id=label2id)\n",
        "model.to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ZrOF0HIpuLs",
        "outputId": "b0fe3c83-1575-4f59-f850-fbb534fe64b2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.dense.bias']\n",
            "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RobertaForTokenClassification(\n",
              "  (roberta): RobertaModel(\n",
              "    (embeddings): RobertaEmbeddings(\n",
              "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
              "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
              "      (token_type_embeddings): Embedding(1, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): RobertaEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x RobertaLayer(\n",
              "          (attention): RobertaAttention(\n",
              "            (self): RobertaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): RobertaSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): RobertaIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): RobertaOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=17, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "Duj63rrqqA4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids = training_set[0][\"ids\"].unsqueeze(0)\n",
        "mask = training_set[0][\"mask\"].unsqueeze(0)\n",
        "targets = training_set[0][\"targets\"].unsqueeze(0)\n",
        "ids = ids.to(device)\n",
        "mask = mask.to(device)\n",
        "targets = targets.to(device)\n",
        "outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "initial_loss = outputs[0]\n",
        "initial_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rVwZzuRew9U",
        "outputId": "fe1d8053-fd31-4248-bce5-0d3b3b2c55a1"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.7136, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tr_logits = outputs[1]\n",
        "tr_logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYwfqZ4Beu8n",
        "outputId": "582a4b40-62e5-4eb0-c468-1adb0e25bc27"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 17])"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "8iWVanMlp95v"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "    # put model in training mode\n",
        "    model.train()\n",
        "\n",
        "    for idx, batch in enumerate(training_loader):\n",
        "        \n",
        "        ids = batch['ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['mask'].to(device, dtype = torch.long)\n",
        "        targets = batch['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "        loss, tr_logits = outputs.loss, outputs.logits\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += targets.size(0)\n",
        "        \n",
        "        if idx % 100==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
        "\n",
        "        # compute training accuracy\n",
        "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
        "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
        "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "        \n",
        "        tr_preds.extend(predictions)\n",
        "        tr_labels.extend(targets)\n",
        "        \n",
        "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "    \n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
        "        )\n",
        "        \n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
      ],
      "metadata": {
        "id": "QRyvWVYNm972"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop for model\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Training epoch: {epoch + 1}\")\n",
        "    train(epoch)"
      ],
      "metadata": {
        "id": "e8ojTxqKqJwL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb1e4ecd-6226-489d-d4aa-34f5dd147590"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training epoch: 1\n",
            "Training loss per 100 training steps: 2.685727119445801\n",
            "Training loss epoch: 2.5613812804222107\n",
            "Training accuracy epoch: 0.22818146433408845\n",
            "Training epoch: 2\n",
            "Training loss per 100 training steps: 2.3390085697174072\n",
            "Training loss epoch: 2.1723614931106567\n",
            "Training accuracy epoch: 0.3351521083424971\n",
            "Training epoch: 3\n",
            "Training loss per 100 training steps: 1.8766632080078125\n",
            "Training loss epoch: 1.8609563112258911\n",
            "Training accuracy epoch: 0.3354721453086417\n",
            "Training epoch: 4\n",
            "Training loss per 100 training steps: 1.7352080345153809\n",
            "Training loss epoch: 1.6827155649662018\n",
            "Training accuracy epoch: 0.36198072518559543\n",
            "Training epoch: 5\n",
            "Training loss per 100 training steps: 1.6039276123046875\n",
            "Training loss epoch: 1.5545009970664978\n",
            "Training accuracy epoch: 0.4173189608224922\n",
            "Training epoch: 6\n",
            "Training loss per 100 training steps: 1.448981761932373\n",
            "Training loss epoch: 1.4206725656986237\n",
            "Training accuracy epoch: 0.47247360009096523\n",
            "Training epoch: 7\n",
            "Training loss per 100 training steps: 1.275575876235962\n",
            "Training loss epoch: 1.3037825226783752\n",
            "Training accuracy epoch: 0.5335728273046967\n",
            "Training epoch: 8\n",
            "Training loss per 100 training steps: 1.1760883331298828\n",
            "Training loss epoch: 1.189727008342743\n",
            "Training accuracy epoch: 0.5759213221163493\n",
            "Training epoch: 9\n",
            "Training loss per 100 training steps: 1.076803207397461\n",
            "Training loss epoch: 1.0775099098682404\n",
            "Training accuracy epoch: 0.6010396092868937\n",
            "Training epoch: 10\n",
            "Training loss per 100 training steps: 1.0074106454849243\n",
            "Training loss epoch: 0.9685713052749634\n",
            "Training accuracy epoch: 0.6415853272539953\n",
            "Training epoch: 11\n",
            "Training loss per 100 training steps: 0.9313490390777588\n",
            "Training loss epoch: 0.864517405629158\n",
            "Training accuracy epoch: 0.6901103688541186\n",
            "Training epoch: 12\n",
            "Training loss per 100 training steps: 0.822469174861908\n",
            "Training loss epoch: 0.7703401446342468\n",
            "Training accuracy epoch: 0.7286622521878128\n",
            "Training epoch: 13\n",
            "Training loss per 100 training steps: 0.7129755616188049\n",
            "Training loss epoch: 0.6832004338502884\n",
            "Training accuracy epoch: 0.7592247806810464\n",
            "Training epoch: 14\n",
            "Training loss per 100 training steps: 0.6312713623046875\n",
            "Training loss epoch: 0.6133835166692734\n",
            "Training accuracy epoch: 0.7842115697645927\n",
            "Training epoch: 15\n",
            "Training loss per 100 training steps: 0.5471149682998657\n",
            "Training loss epoch: 0.5536224693059921\n",
            "Training accuracy epoch: 0.8046680467110702\n",
            "Training epoch: 16\n",
            "Training loss per 100 training steps: 0.5189379453659058\n",
            "Training loss epoch: 0.5022607669234276\n",
            "Training accuracy epoch: 0.8268973962851465\n",
            "Training epoch: 17\n",
            "Training loss per 100 training steps: 0.45924463868141174\n",
            "Training loss epoch: 0.4522469639778137\n",
            "Training accuracy epoch: 0.8484075930340256\n",
            "Training epoch: 18\n",
            "Training loss per 100 training steps: 0.43102458119392395\n",
            "Training loss epoch: 0.41144561022520065\n",
            "Training accuracy epoch: 0.8635793658982995\n",
            "Training epoch: 19\n",
            "Training loss per 100 training steps: 0.40982216596603394\n",
            "Training loss epoch: 0.37562622874975204\n",
            "Training accuracy epoch: 0.8797176278183964\n",
            "Training epoch: 20\n",
            "Training loss per 100 training steps: 0.33783063292503357\n",
            "Training loss epoch: 0.33950652927160263\n",
            "Training accuracy epoch: 0.8932010070088124\n",
            "Training epoch: 21\n",
            "Training loss per 100 training steps: 0.32702603936195374\n",
            "Training loss epoch: 0.30974842607975006\n",
            "Training accuracy epoch: 0.9054535588507997\n",
            "Training epoch: 22\n",
            "Training loss per 100 training steps: 0.2976425886154175\n",
            "Training loss epoch: 0.28335341066122055\n",
            "Training accuracy epoch: 0.9187366753572659\n",
            "Training epoch: 23\n",
            "Training loss per 100 training steps: 0.2834613621234894\n",
            "Training loss epoch: 0.2577616199851036\n",
            "Training accuracy epoch: 0.9254243306419949\n",
            "Training epoch: 24\n",
            "Training loss per 100 training steps: 0.2545284628868103\n",
            "Training loss epoch: 0.23769984766840935\n",
            "Training accuracy epoch: 0.9292108260540939\n",
            "Training epoch: 25\n",
            "Training loss per 100 training steps: 0.22645540535449982\n",
            "Training loss epoch: 0.22298454493284225\n",
            "Training accuracy epoch: 0.9342782440831766\n",
            "Training epoch: 26\n",
            "Training loss per 100 training steps: 0.24341365694999695\n",
            "Training loss epoch: 0.20655172318220139\n",
            "Training accuracy epoch: 0.9394140801530083\n",
            "Training epoch: 27\n",
            "Training loss per 100 training steps: 0.18448826670646667\n",
            "Training loss epoch: 0.19347528368234634\n",
            "Training accuracy epoch: 0.94205472631203\n",
            "Training epoch: 28\n",
            "Training loss per 100 training steps: 0.1902453601360321\n",
            "Training loss epoch: 0.18340517953038216\n",
            "Training accuracy epoch: 0.9454690524615741\n",
            "Training epoch: 29\n",
            "Training loss per 100 training steps: 0.16200703382492065\n",
            "Training loss epoch: 0.16877305880188942\n",
            "Training accuracy epoch: 0.9501928666582218\n",
            "Training epoch: 30\n",
            "Training loss per 100 training steps: 0.1633882075548172\n",
            "Training loss epoch: 0.1623622179031372\n",
            "Training accuracy epoch: 0.9525314064658815\n",
            "Training epoch: 31\n",
            "Training loss per 100 training steps: 0.13112537562847137\n",
            "Training loss epoch: 0.15163186937570572\n",
            "Training accuracy epoch: 0.9560571804466648\n",
            "Training epoch: 32\n",
            "Training loss per 100 training steps: 0.14250707626342773\n",
            "Training loss epoch: 0.14253505691885948\n",
            "Training accuracy epoch: 0.9580630277132268\n",
            "Training epoch: 33\n",
            "Training loss per 100 training steps: 0.1387483775615692\n",
            "Training loss epoch: 0.13516063429415226\n",
            "Training accuracy epoch: 0.9601005264286046\n",
            "Training epoch: 34\n",
            "Training loss per 100 training steps: 0.14754578471183777\n",
            "Training loss epoch: 0.13127667643129826\n",
            "Training accuracy epoch: 0.9609813328879575\n",
            "Training epoch: 35\n",
            "Training loss per 100 training steps: 0.11638350039720535\n",
            "Training loss epoch: 0.12192556820809841\n",
            "Training accuracy epoch: 0.9648364572858993\n",
            "Training epoch: 36\n",
            "Training loss per 100 training steps: 0.12835480272769928\n",
            "Training loss epoch: 0.11632681079208851\n",
            "Training accuracy epoch: 0.9668808565349077\n",
            "Training epoch: 37\n",
            "Training loss per 100 training steps: 0.10382875055074692\n",
            "Training loss epoch: 0.11109059490263462\n",
            "Training accuracy epoch: 0.9679538570310833\n",
            "Training epoch: 38\n",
            "Training loss per 100 training steps: 0.11197181791067123\n",
            "Training loss epoch: 0.10547293908894062\n",
            "Training accuracy epoch: 0.9691169831806401\n",
            "Training epoch: 39\n",
            "Training loss per 100 training steps: 0.09920582175254822\n",
            "Training loss epoch: 0.10035707615315914\n",
            "Training accuracy epoch: 0.9709247989881691\n",
            "Training epoch: 40\n",
            "Training loss per 100 training steps: 0.09905211627483368\n",
            "Training loss epoch: 0.09777358174324036\n",
            "Training accuracy epoch: 0.9717939620847159\n",
            "Training epoch: 41\n",
            "Training loss per 100 training steps: 0.09704835712909698\n",
            "Training loss epoch: 0.09226315096020699\n",
            "Training accuracy epoch: 0.9735394125180025\n",
            "Training epoch: 42\n",
            "Training loss per 100 training steps: 0.08325935900211334\n",
            "Training loss epoch: 0.08951776660978794\n",
            "Training accuracy epoch: 0.9737555333000887\n",
            "Training epoch: 43\n",
            "Training loss per 100 training steps: 0.0913204476237297\n",
            "Training loss epoch: 0.08771517314016819\n",
            "Training accuracy epoch: 0.9750867410312644\n",
            "Training epoch: 44\n",
            "Training loss per 100 training steps: 0.07741449773311615\n",
            "Training loss epoch: 0.08441462740302086\n",
            "Training accuracy epoch: 0.975555738877123\n",
            "Training epoch: 45\n",
            "Training loss per 100 training steps: 0.08839122205972672\n",
            "Training loss epoch: 0.0801639910787344\n",
            "Training accuracy epoch: 0.9777278155498904\n",
            "Training epoch: 46\n",
            "Training loss per 100 training steps: 0.0717911496758461\n",
            "Training loss epoch: 0.07671401277184486\n",
            "Training accuracy epoch: 0.9781623648441548\n",
            "Training epoch: 47\n",
            "Training loss per 100 training steps: 0.07159283012151718\n",
            "Training loss epoch: 0.07391735911369324\n",
            "Training accuracy epoch: 0.9795514401288619\n",
            "Training epoch: 48\n",
            "Training loss per 100 training steps: 0.06677679717540741\n",
            "Training loss epoch: 0.07062234543263912\n",
            "Training accuracy epoch: 0.9795158518892786\n",
            "Training epoch: 49\n",
            "Training loss per 100 training steps: 0.0627075582742691\n",
            "Training loss epoch: 0.06635818630456924\n",
            "Training accuracy epoch: 0.9811906500516161\n",
            "Training epoch: 50\n",
            "Training loss per 100 training steps: 0.06417877972126007\n",
            "Training loss epoch: 0.06776725128293037\n",
            "Training accuracy epoch: 0.980876354361955\n",
            "Training epoch: 51\n",
            "Training loss per 100 training steps: 0.0680292546749115\n",
            "Training loss epoch: 0.06463276222348213\n",
            "Training accuracy epoch: 0.9807287504851967\n",
            "Training epoch: 52\n",
            "Training loss per 100 training steps: 0.06425207108259201\n",
            "Training loss epoch: 0.061895979568362236\n",
            "Training accuracy epoch: 0.9824911022491196\n",
            "Training epoch: 53\n",
            "Training loss per 100 training steps: 0.054875511676073074\n",
            "Training loss epoch: 0.06229534838348627\n",
            "Training accuracy epoch: 0.9820482311821355\n",
            "Training epoch: 54\n",
            "Training loss per 100 training steps: 0.06780984997749329\n",
            "Training loss epoch: 0.060283856466412544\n",
            "Training accuracy epoch: 0.9833132455515246\n",
            "Training epoch: 55\n",
            "Training loss per 100 training steps: 0.05874130502343178\n",
            "Training loss epoch: 0.059267351403832436\n",
            "Training accuracy epoch: 0.983523974474455\n",
            "Training epoch: 56\n",
            "Training loss per 100 training steps: 0.05884924903512001\n",
            "Training loss epoch: 0.05569597240537405\n",
            "Training accuracy epoch: 0.9845907066479165\n",
            "Training epoch: 57\n",
            "Training loss per 100 training steps: 0.05029747262597084\n",
            "Training loss epoch: 0.056053527630865574\n",
            "Training accuracy epoch: 0.9838475718084022\n",
            "Training epoch: 58\n",
            "Training loss per 100 training steps: 0.051211219280958176\n",
            "Training loss epoch: 0.05334856081753969\n",
            "Training accuracy epoch: 0.9844889860523212\n",
            "Training epoch: 59\n",
            "Training loss per 100 training steps: 0.048438336700201035\n",
            "Training loss epoch: 0.05050046369433403\n",
            "Training accuracy epoch: 0.9861643908970419\n",
            "Training epoch: 60\n",
            "Training loss per 100 training steps: 0.05163271352648735\n",
            "Training loss epoch: 0.0479694502428174\n",
            "Training accuracy epoch: 0.9865787391301754\n",
            "Training epoch: 61\n",
            "Training loss per 100 training steps: 0.046922359615564346\n",
            "Training loss epoch: 0.04676710907369852\n",
            "Training accuracy epoch: 0.9870783485814779\n",
            "Training epoch: 62\n",
            "Training loss per 100 training steps: 0.043407127261161804\n",
            "Training loss epoch: 0.04715009778738022\n",
            "Training accuracy epoch: 0.9869037290443697\n",
            "Training epoch: 63\n",
            "Training loss per 100 training steps: 0.04623023048043251\n",
            "Training loss epoch: 0.04758244659751654\n",
            "Training accuracy epoch: 0.98697934115603\n",
            "Training epoch: 64\n",
            "Training loss per 100 training steps: 0.04552721604704857\n",
            "Training loss epoch: 0.04338647238910198\n",
            "Training accuracy epoch: 0.9880484226198349\n",
            "Training epoch: 65\n",
            "Training loss per 100 training steps: 0.04807724058628082\n",
            "Training loss epoch: 0.04356382321566343\n",
            "Training accuracy epoch: 0.9879180694857674\n",
            "Training epoch: 66\n",
            "Training loss per 100 training steps: 0.045254044234752655\n",
            "Training loss epoch: 0.04289411939680576\n",
            "Training accuracy epoch: 0.9882995675428252\n",
            "Training epoch: 67\n",
            "Training loss per 100 training steps: 0.03774194419384003\n",
            "Training loss epoch: 0.041080111637711525\n",
            "Training accuracy epoch: 0.9885528316121193\n",
            "Training epoch: 68\n",
            "Training loss per 100 training steps: 0.043774645775556564\n",
            "Training loss epoch: 0.039412911515682936\n",
            "Training accuracy epoch: 0.9895195524118579\n",
            "Training epoch: 69\n",
            "Training loss per 100 training steps: 0.04132328927516937\n",
            "Training loss epoch: 0.038471982814371586\n",
            "Training accuracy epoch: 0.9893218867747162\n",
            "Training epoch: 70\n",
            "Training loss per 100 training steps: 0.03186529502272606\n",
            "Training loss epoch: 0.03580536134541035\n",
            "Training accuracy epoch: 0.9907393665478978\n",
            "Training epoch: 71\n",
            "Training loss per 100 training steps: 0.03295542672276497\n",
            "Training loss epoch: 0.037076136097311974\n",
            "Training accuracy epoch: 0.9898203902777363\n",
            "Training epoch: 72\n",
            "Training loss per 100 training steps: 0.038297105580568314\n",
            "Training loss epoch: 0.03555816877633333\n",
            "Training accuracy epoch: 0.9906972675572518\n",
            "Training epoch: 73\n",
            "Training loss per 100 training steps: 0.03620634227991104\n",
            "Training loss epoch: 0.03563220566138625\n",
            "Training accuracy epoch: 0.9909373391194756\n",
            "Training epoch: 74\n",
            "Training loss per 100 training steps: 0.039621297270059586\n",
            "Training loss epoch: 0.033459366764873266\n",
            "Training accuracy epoch: 0.9917278155715102\n",
            "Training epoch: 75\n",
            "Training loss per 100 training steps: 0.03513843193650246\n",
            "Training loss epoch: 0.03518908750265837\n",
            "Training accuracy epoch: 0.9902782138484881\n",
            "Training epoch: 76\n",
            "Training loss per 100 training steps: 0.036198146641254425\n",
            "Training loss epoch: 0.03391243889927864\n",
            "Training accuracy epoch: 0.9911466127700566\n",
            "Training epoch: 77\n",
            "Training loss per 100 training steps: 0.03375433385372162\n",
            "Training loss epoch: 0.03248621663078666\n",
            "Training accuracy epoch: 0.9918895188228903\n",
            "Training epoch: 78\n",
            "Training loss per 100 training steps: 0.03266393393278122\n",
            "Training loss epoch: 0.031886203680187464\n",
            "Training accuracy epoch: 0.9917134230428245\n",
            "Training epoch: 79\n",
            "Training loss per 100 training steps: 0.02503405325114727\n",
            "Training loss epoch: 0.030876217875629663\n",
            "Training accuracy epoch: 0.9920103479383756\n",
            "Training epoch: 80\n",
            "Training loss per 100 training steps: 0.03103744611144066\n",
            "Training loss epoch: 0.032672333531081676\n",
            "Training accuracy epoch: 0.9916453880676959\n",
            "Training epoch: 81\n",
            "Training loss per 100 training steps: 0.02694040909409523\n",
            "Training loss epoch: 0.031327687203884125\n",
            "Training accuracy epoch: 0.9915518281775664\n",
            "Training epoch: 82\n",
            "Training loss per 100 training steps: 0.03386343643069267\n",
            "Training loss epoch: 0.03030338790267706\n",
            "Training accuracy epoch: 0.992528204468906\n",
            "Training epoch: 83\n",
            "Training loss per 100 training steps: 0.02863703854382038\n",
            "Training loss epoch: 0.0283828298561275\n",
            "Training accuracy epoch: 0.9930632477301129\n",
            "Training epoch: 84\n",
            "Training loss per 100 training steps: 0.027142507955431938\n",
            "Training loss epoch: 0.028474764432758093\n",
            "Training accuracy epoch: 0.9928994595797321\n",
            "Training epoch: 85\n",
            "Training loss per 100 training steps: 0.027712136507034302\n",
            "Training loss epoch: 0.027937698177993298\n",
            "Training accuracy epoch: 0.9928723723997066\n",
            "Training epoch: 86\n",
            "Training loss per 100 training steps: 0.02797686867415905\n",
            "Training loss epoch: 0.029077925719320774\n",
            "Training accuracy epoch: 0.9928534686840813\n",
            "Training epoch: 87\n",
            "Training loss per 100 training steps: 0.028704684227705002\n",
            "Training loss epoch: 0.02744913613423705\n",
            "Training accuracy epoch: 0.9931498268814714\n",
            "Training epoch: 88\n",
            "Training loss per 100 training steps: 0.0218215249478817\n",
            "Training loss epoch: 0.026303648483008146\n",
            "Training accuracy epoch: 0.993228682136434\n",
            "Training epoch: 89\n",
            "Training loss per 100 training steps: 0.02457568794488907\n",
            "Training loss epoch: 0.027167906519025564\n",
            "Training accuracy epoch: 0.9930786097883566\n",
            "Training epoch: 90\n",
            "Training loss per 100 training steps: 0.030195625498890877\n",
            "Training loss epoch: 0.026709153782576323\n",
            "Training accuracy epoch: 0.9928344740922604\n",
            "Training epoch: 91\n",
            "Training loss per 100 training steps: 0.025068508461117744\n",
            "Training loss epoch: 0.026529083494096994\n",
            "Training accuracy epoch: 0.992920631028821\n",
            "Training epoch: 92\n",
            "Training loss per 100 training steps: 0.027149325236678123\n",
            "Training loss epoch: 0.02661986369639635\n",
            "Training accuracy epoch: 0.9930429730146062\n",
            "Training epoch: 93\n",
            "Training loss per 100 training steps: 0.020607607439160347\n",
            "Training loss epoch: 0.023914539255201817\n",
            "Training accuracy epoch: 0.9940868985414024\n",
            "Training epoch: 94\n",
            "Training loss per 100 training steps: 0.02779686264693737\n",
            "Training loss epoch: 0.02553930412977934\n",
            "Training accuracy epoch: 0.9936097267518706\n",
            "Training epoch: 95\n",
            "Training loss per 100 training steps: 0.030634919181466103\n",
            "Training loss epoch: 0.025459437631070614\n",
            "Training accuracy epoch: 0.9929148136645776\n",
            "Training epoch: 96\n",
            "Training loss per 100 training steps: 0.0218034740537405\n",
            "Training loss epoch: 0.023897618055343628\n",
            "Training accuracy epoch: 0.9938072502939628\n",
            "Training epoch: 97\n",
            "Training loss per 100 training steps: 0.031827107071876526\n",
            "Training loss epoch: 0.024252041708678007\n",
            "Training accuracy epoch: 0.9936871164003568\n",
            "Training epoch: 98\n",
            "Training loss per 100 training steps: 0.028565583750605583\n",
            "Training loss epoch: 0.024443854577839375\n",
            "Training accuracy epoch: 0.9936665085399126\n",
            "Training epoch: 99\n",
            "Training loss per 100 training steps: 0.025526797398924828\n",
            "Training loss epoch: 0.024149771314114332\n",
            "Training accuracy epoch: 0.9937737457932403\n",
            "Training epoch: 100\n",
            "Training loss per 100 training steps: 0.020771648734807968\n",
            "Training loss epoch: 0.02246871218085289\n",
            "Training accuracy epoch: 0.994355295593906\n",
            "Training epoch: 101\n",
            "Training loss per 100 training steps: 0.02110179327428341\n",
            "Training loss epoch: 0.022162743844091892\n",
            "Training accuracy epoch: 0.9946638646788322\n",
            "Training epoch: 102\n",
            "Training loss per 100 training steps: 0.016765374690294266\n",
            "Training loss epoch: 0.021605865564197302\n",
            "Training accuracy epoch: 0.994312149625298\n",
            "Training epoch: 103\n",
            "Training loss per 100 training steps: 0.02083214558660984\n",
            "Training loss epoch: 0.022850946057587862\n",
            "Training accuracy epoch: 0.9941246841716385\n",
            "Training epoch: 104\n",
            "Training loss per 100 training steps: 0.022616451606154442\n",
            "Training loss epoch: 0.02427614713087678\n",
            "Training accuracy epoch: 0.993272553180668\n",
            "Training epoch: 105\n",
            "Training loss per 100 training steps: 0.026021333411335945\n",
            "Training loss epoch: 0.022892939392477274\n",
            "Training accuracy epoch: 0.9934828369381586\n",
            "Training epoch: 106\n",
            "Training loss per 100 training steps: 0.027986733242869377\n",
            "Training loss epoch: 0.023695374839007854\n",
            "Training accuracy epoch: 0.9935033479615423\n",
            "Training epoch: 107\n",
            "Training loss per 100 training steps: 0.018451420590281487\n",
            "Training loss epoch: 0.022656457498669624\n",
            "Training accuracy epoch: 0.9935344373987763\n",
            "Training epoch: 108\n",
            "Training loss per 100 training steps: 0.023033328354358673\n",
            "Training loss epoch: 0.021350072231143713\n",
            "Training accuracy epoch: 0.9944580004412791\n",
            "Training epoch: 109\n",
            "Training loss per 100 training steps: 0.01882358267903328\n",
            "Training loss epoch: 0.021540229208767414\n",
            "Training accuracy epoch: 0.9941686168240449\n",
            "Training epoch: 110\n",
            "Training loss per 100 training steps: 0.01813790574669838\n",
            "Training loss epoch: 0.021897378843277693\n",
            "Training accuracy epoch: 0.994016049506537\n",
            "Training epoch: 111\n",
            "Training loss per 100 training steps: 0.017402607947587967\n",
            "Training loss epoch: 0.01917673577554524\n",
            "Training accuracy epoch: 0.9949789940431761\n",
            "Training epoch: 112\n",
            "Training loss per 100 training steps: 0.017545491456985474\n",
            "Training loss epoch: 0.018383936025202274\n",
            "Training accuracy epoch: 0.9951294696619429\n",
            "Training epoch: 113\n",
            "Training loss per 100 training steps: 0.02178320474922657\n",
            "Training loss epoch: 0.020101310685276985\n",
            "Training accuracy epoch: 0.9941932000846143\n",
            "Training epoch: 114\n",
            "Training loss per 100 training steps: 0.017040664330124855\n",
            "Training loss epoch: 0.019034162629395723\n",
            "Training accuracy epoch: 0.994789689485018\n",
            "Training epoch: 115\n",
            "Training loss per 100 training steps: 0.013937185518443584\n",
            "Training loss epoch: 0.018020633840933442\n",
            "Training accuracy epoch: 0.995266181083204\n",
            "Training epoch: 116\n",
            "Training loss per 100 training steps: 0.017531294375658035\n",
            "Training loss epoch: 0.017922656843438745\n",
            "Training accuracy epoch: 0.9952698810148365\n",
            "Training epoch: 117\n",
            "Training loss per 100 training steps: 0.015527712181210518\n",
            "Training loss epoch: 0.017331704031676054\n",
            "Training accuracy epoch: 0.9956713531423125\n",
            "Training epoch: 118\n",
            "Training loss per 100 training steps: 0.020476283505558968\n",
            "Training loss epoch: 0.018346548778936267\n",
            "Training accuracy epoch: 0.9952958111156303\n",
            "Training epoch: 119\n",
            "Training loss per 100 training steps: 0.014997062273323536\n",
            "Training loss epoch: 0.01760860183276236\n",
            "Training accuracy epoch: 0.9954684357166057\n",
            "Training epoch: 120\n",
            "Training loss per 100 training steps: 0.014861968345940113\n",
            "Training loss epoch: 0.01780458423309028\n",
            "Training accuracy epoch: 0.9951539666747439\n",
            "Training epoch: 121\n",
            "Training loss per 100 training steps: 0.01775222271680832\n",
            "Training loss epoch: 0.01715505449101329\n",
            "Training accuracy epoch: 0.9955972210580046\n",
            "Training epoch: 122\n",
            "Training loss per 100 training steps: 0.012497265823185444\n",
            "Training loss epoch: 0.01686634705401957\n",
            "Training accuracy epoch: 0.9957806105871136\n",
            "Training epoch: 123\n",
            "Training loss per 100 training steps: 0.01425244566053152\n",
            "Training loss epoch: 0.01607743790373206\n",
            "Training accuracy epoch: 0.9959098644304551\n",
            "Training epoch: 124\n",
            "Training loss per 100 training steps: 0.012920080684125423\n",
            "Training loss epoch: 0.016718961531296372\n",
            "Training accuracy epoch: 0.9953045699437124\n",
            "Training epoch: 125\n",
            "Training loss per 100 training steps: 0.013547644019126892\n",
            "Training loss epoch: 0.01732968306168914\n",
            "Training accuracy epoch: 0.9954157165495601\n",
            "Training epoch: 126\n",
            "Training loss per 100 training steps: 0.017657628282904625\n",
            "Training loss epoch: 0.01598697411827743\n",
            "Training accuracy epoch: 0.9954697614571966\n",
            "Training epoch: 127\n",
            "Training loss per 100 training steps: 0.010550180450081825\n",
            "Training loss epoch: 0.016123612178489566\n",
            "Training accuracy epoch: 0.9955342683245227\n",
            "Training epoch: 128\n",
            "Training loss per 100 training steps: 0.012584123760461807\n",
            "Training loss epoch: 0.014686566777527332\n",
            "Training accuracy epoch: 0.995916453084547\n",
            "Training epoch: 129\n",
            "Training loss per 100 training steps: 0.015032473020255566\n",
            "Training loss epoch: 0.015830852556973696\n",
            "Training accuracy epoch: 0.9955039493303199\n",
            "Training epoch: 130\n",
            "Training loss per 100 training steps: 0.013463811948895454\n",
            "Training loss epoch: 0.016112568089738488\n",
            "Training accuracy epoch: 0.995614830496213\n",
            "Training epoch: 131\n",
            "Training loss per 100 training steps: 0.0188909899443388\n",
            "Training loss epoch: 0.01535456907004118\n",
            "Training accuracy epoch: 0.9961431263845778\n",
            "Training epoch: 132\n",
            "Training loss per 100 training steps: 0.0075550684705376625\n",
            "Training loss epoch: 0.014398046769201756\n",
            "Training accuracy epoch: 0.9962262219388403\n",
            "Training epoch: 133\n",
            "Training loss per 100 training steps: 0.01464829221367836\n",
            "Training loss epoch: 0.014949167845770717\n",
            "Training accuracy epoch: 0.9958190658250667\n",
            "Training epoch: 134\n",
            "Training loss per 100 training steps: 0.016522306948900223\n",
            "Training loss epoch: 0.01400135480798781\n",
            "Training accuracy epoch: 0.9961808011393898\n",
            "Training epoch: 135\n",
            "Training loss per 100 training steps: 0.01206644345074892\n",
            "Training loss epoch: 0.01405829330906272\n",
            "Training accuracy epoch: 0.9966667604984322\n",
            "Training epoch: 136\n",
            "Training loss per 100 training steps: 0.014420497231185436\n",
            "Training loss epoch: 0.01384922955185175\n",
            "Training accuracy epoch: 0.9963179621152957\n",
            "Training epoch: 137\n",
            "Training loss per 100 training steps: 0.015177461318671703\n",
            "Training loss epoch: 0.013713354710489511\n",
            "Training accuracy epoch: 0.9965893638947807\n",
            "Training epoch: 138\n",
            "Training loss per 100 training steps: 0.014865218661725521\n",
            "Training loss epoch: 0.013837373815476894\n",
            "Training accuracy epoch: 0.9965693351786602\n",
            "Training epoch: 139\n",
            "Training loss per 100 training steps: 0.01103214267641306\n",
            "Training loss epoch: 0.013017448130995035\n",
            "Training accuracy epoch: 0.9964630952172723\n",
            "Training epoch: 140\n",
            "Training loss per 100 training steps: 0.01301433052867651\n",
            "Training loss epoch: 0.013974720146507025\n",
            "Training accuracy epoch: 0.9963374772235293\n",
            "Training epoch: 141\n",
            "Training loss per 100 training steps: 0.009756172075867653\n",
            "Training loss epoch: 0.01254371739923954\n",
            "Training accuracy epoch: 0.9968245897097723\n",
            "Training epoch: 142\n",
            "Training loss per 100 training steps: 0.00917646661400795\n",
            "Training loss epoch: 0.012410731753334403\n",
            "Training accuracy epoch: 0.9968397079408252\n",
            "Training epoch: 143\n",
            "Training loss per 100 training steps: 0.017384618520736694\n",
            "Training loss epoch: 0.013830344192683697\n",
            "Training accuracy epoch: 0.996467793229171\n",
            "Training epoch: 144\n",
            "Training loss per 100 training steps: 0.013650944456458092\n",
            "Training loss epoch: 0.01253375131636858\n",
            "Training accuracy epoch: 0.9966486768742752\n",
            "Training epoch: 145\n",
            "Training loss per 100 training steps: 0.011753673665225506\n",
            "Training loss epoch: 0.011932942783460021\n",
            "Training accuracy epoch: 0.9971685037629804\n",
            "Training epoch: 146\n",
            "Training loss per 100 training steps: 0.013363511301577091\n",
            "Training loss epoch: 0.012328706216067076\n",
            "Training accuracy epoch: 0.9967660464207281\n",
            "Training epoch: 147\n",
            "Training loss per 100 training steps: 0.012099809013307095\n",
            "Training loss epoch: 0.012653658399358392\n",
            "Training accuracy epoch: 0.9962712641666097\n",
            "Training epoch: 148\n",
            "Training loss per 100 training steps: 0.010627475567162037\n",
            "Training loss epoch: 0.01275007682852447\n",
            "Training accuracy epoch: 0.9967042366787527\n",
            "Training epoch: 149\n",
            "Training loss per 100 training steps: 0.011575429700314999\n",
            "Training loss epoch: 0.012059038504958153\n",
            "Training accuracy epoch: 0.9968314433924642\n",
            "Training epoch: 150\n",
            "Training loss per 100 training steps: 0.011972167529165745\n",
            "Training loss epoch: 0.012474836083129048\n",
            "Training accuracy epoch: 0.9962711044319161\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate model**"
      ],
      "metadata": {
        "id": "g2eN1TIrqRWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def valid(model, testing_loader):\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "            \n",
        "            ids = batch['ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['mask'].to(device, dtype = torch.long)\n",
        "            targets = batch['targets'].to(device, dtype = torch.long)\n",
        "            \n",
        "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "            loss, eval_logits = outputs.loss, outputs.logits\n",
        "            \n",
        "            eval_loss += loss.item()\n",
        "\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += targets.size(0)\n",
        "        \n",
        "            if idx % 100==0:\n",
        "                loss_step = eval_loss/nb_eval_steps\n",
        "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
        "              \n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
        "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            \n",
        "            eval_labels.extend(targets)\n",
        "            eval_preds.extend(predictions)\n",
        "            \n",
        "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "    \n",
        "    #print(eval_labels)\n",
        "    #print(eval_preds)\n",
        "\n",
        "    labels = [id2label[id.item()] for id in eval_labels]\n",
        "    predictions = [id2label[id.item()] for id in eval_preds]\n",
        "\n",
        "    #print(labels)\n",
        "    #print(predictions)\n",
        "    \n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return labels, predictions"
      ],
      "metadata": {
        "id": "Bcp34yC1qTYw"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels, predictions = valid(model, testing_loader)"
      ],
      "metadata": {
        "id": "eo-FdVhJqV__",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4b5c975-6881-417d-c069-3b4f158cf887"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation loss per 100 evaluation steps: 0.07212091982364655\n",
            "Validation Loss: 0.10238920152187347\n",
            "Validation Accuracy: 0.9791171746451011\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "print(classification_report([labels], [predictions]))"
      ],
      "metadata": {
        "id": "WPwx1qVpqdFE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f598ea7-f002-4624-d2b0-cd9967657cbd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "        COMM       0.83      0.83      0.83        18\n",
            "        DATE       0.99      1.00      0.99       246\n",
            "        INFO       0.97      0.99      0.98       235\n",
            "         LOC       0.89      0.91      0.90       296\n",
            "        MISC       0.89      0.94      0.92       196\n",
            "         NUM       1.00      1.00      1.00       326\n",
            "         OCC       0.92      0.96      0.94       227\n",
            "         PER       0.99      0.98      0.98       483\n",
            "\n",
            "   micro avg       0.95      0.97      0.96      2027\n",
            "   macro avg       0.94      0.95      0.94      2027\n",
            "weighted avg       0.96      0.97      0.96      2027\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Save model weights and tokenizer locally**"
      ],
      "metadata": {
        "id": "VfhsUI2N8uHI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = \"/path/to/directory\"\n",
        "\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ],
      "metadata": {
        "id": "gVHet0kG8vkO",
        "outputId": "bb56153d-5242-4588-f632-549484e2b6b5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/MyDrive/directory/tokenizer_config.json',\n",
              " '/content/drive/MyDrive/directory/special_tokens_map.json',\n",
              " '/content/drive/MyDrive/directory/vocab.json',\n",
              " '/content/drive/MyDrive/directory/merges.txt',\n",
              " '/content/drive/MyDrive/directory/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once you stop training, you **must** save the contents of the outputs folder to your local directory (Colab deletes local files once the runtime is deleted)."
      ],
      "metadata": {
        "id": "cfrktDih3ZCJ"
      }
    }
  ]
}