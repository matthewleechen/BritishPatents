{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V100",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/matthewleechen/woodcroft_patents/blob/main/ner/notebooks/ner_patents.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is a modified version of Niels Rogge's (extremely helpful!) notebook, \"Fine-tuning BERT for named entity recognition\", linked [here](https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT.ipynb). \n",
        "\n",
        "It is **not** recommended to run this notebook on the Colab free plan. This notebook's training loop was originally run using Colab Pro on 1 Nvidia Tesla V100 (16GB) GPU. You can also run this locally on a virtual machine or server, but carefully check for dependencies.\n",
        "\n",
        "This notebook uses the BERT model 'bert-base-uncased' (linked [here](https://huggingface.co/bert-base-uncased)), but any pre-trained model on [HuggingFace](https://huggingface.co) can be used. I have experimented with BERT (base and both cased/uncased), RoBERTa (base and distilled), XLNet (base) and SBERT models (fine-tuned MPNet) and find no meaningful differences in performance on the patents data given the hyperparameters set below (validation accuracy 97-98%)."
      ],
      "metadata": {
        "id": "691hIbZN-Fvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Setup**"
      ],
      "metadata": {
        "id": "oMmMZuroBvoL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers seqeval[gpu]\n",
        "!pip install conllu"
      ],
      "metadata": {
        "id": "OtR8-rnn8frR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import set_seed, AutoConfig, AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
        "import conllu\n",
        "import csv\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "_KagjDqfbPSQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "from torch import cuda\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "print(device)"
      ],
      "metadata": {
        "id": "fy7-Z_OTbWNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee6b0a0e-27b3-414d-c292-dcc43d918726"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set seed\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "Nz8_izzzUPky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preprocessing**"
      ],
      "metadata": {
        "id": "1610Mty_oJ_u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Upload .conll dataset exported from Label Studio"
      ],
      "metadata": {
        "id": "WVK__AbdXtJy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize annotations as .conll dataset\n",
        "data = open(\"/path/to/ner_patents.conll\", mode = \"r\", encoding = \"utf-8\")\n",
        "annotations = data.read()\n",
        "print(annotations[:1000])"
      ],
      "metadata": {
        "id": "-NAddj2PU8K6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define input and output file names\n",
        "input_file = \"ner_patents.conll\" ## assume name of .conll file is ner_patents\n",
        "output_file = \"ner_patents.csv\" ## same here\n",
        "\n",
        "# initialize the csv writer and write the header row\n",
        "csv_writer = csv.writer(open(output_file, \"w\", newline=\"\", encoding=\"utf-8\"))\n",
        "csv_writer.writerow([\"sentence_no\", \"word\", \"tag\"])\n",
        "\n",
        "# read the input file line by line\n",
        "with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    sentence_num = 1\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not line:\n",
        "            # empty line denotes end of sentence\n",
        "            sentence_num += 1\n",
        "        else:\n",
        "            # split line into columns\n",
        "            columns = line.split()\n",
        "            word = columns[0]\n",
        "            tag = columns[-1]\n",
        "            sentence = \"{}\".format(sentence_num)\n",
        "            # write row to csv file\n",
        "            csv_writer.writerow([sentence, word or \"NaN\", tag or \"NaN\"])"
      ],
      "metadata": {
        "id": "amhEpGo4fv5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize data\n",
        "pd.read_csv('ner_patents.csv').drop(0).to_csv('ner_patents.csv', index=False)\n",
        "data = pd.read_csv(\"ner_patents.csv\", encoding='unicode_escape')\n",
        "data.head()"
      ],
      "metadata": {
        "id": "0ROSgiRkj1q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by sentence\n",
        "# let's create a new column called \"patent\" which groups the words by sentence \n",
        "data['sentence'] = data[['sentence_no','word','tag']].groupby(['sentence_no'])['word'].transform(lambda x: ' '.join(x))\n",
        "# let's also create a new column called \"word_labels\" which groups the tags by sentence \n",
        "data['word_labels'] = data[['sentence_no','word','tag']].groupby(['sentence_no'])['tag'].transform(lambda x: ','.join(x))\n",
        "# Show data\n",
        "data.head()"
      ],
      "metadata": {
        "id": "03Eb_UtokFPk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Make dictionary mapping tags to indices\n",
        "label2id = {k: v for v, k in enumerate(data.tag.unique())}\n",
        "id2label = {v: k for v, k in enumerate(data.tag.unique())}\n",
        "label2id"
      ],
      "metadata": {
        "id": "8yTdhHcendoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = data[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n",
        "data.head()"
      ],
      "metadata": {
        "id": "-FsYm5F6nzYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Dataset and Dataloader**"
      ],
      "metadata": {
        "id": "rQScj-PQoTQX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define variables\n",
        "MAX_LEN = 128\n",
        "TRAIN_BATCH_SIZE = 120\n",
        "VALID_BATCH_SIZE = 60\n",
        "EPOCHS = 150\n",
        "LEARNING_RATE = 1e-05\n",
        "MAX_GRAD_NORM = 10\n",
        "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased') # can change tokenizer here"
      ],
      "metadata": {
        "id": "J6AjggpdoWN3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word-level tokenization\n",
        "def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n",
        "    \"\"\"\n",
        "    Word piece tokenization makes it difficult to match word labels\n",
        "    back up with individual word pieces. This function tokenizes each\n",
        "    word one at a time so that it is easier to preserve the correct\n",
        "    label for each subword. It is, of course, a bit slower in processing\n",
        "    time, but it will help our model achieve higher accuracy.\n",
        "    \"\"\"\n",
        "\n",
        "    tokenized_sentence = []\n",
        "    labels = []\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n",
        "\n",
        "        # Tokenize the word and count # of subwords the word is broken into\n",
        "        tokenized_word = tokenizer.tokenize(word)\n",
        "        n_subwords = len(tokenized_word)\n",
        "\n",
        "        # Add the tokenized word to the final tokenized word list\n",
        "        tokenized_sentence.extend(tokenized_word)\n",
        "\n",
        "        # Add the same label to the new list of labels `n_subwords` times\n",
        "        labels.extend([label] * n_subwords)\n",
        "\n",
        "    return tokenized_sentence, labels"
      ],
      "metadata": {
        "id": "IYO8qiEMoYxx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class dataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_len):\n",
        "        self.len = len(dataframe)\n",
        "        self.data = dataframe\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "        \n",
        "    def __getitem__(self, index):\n",
        "        # step 1: tokenize (and adapt corresponding labels)\n",
        "        sentence = self.data.sentence[index]  \n",
        "        word_labels = self.data.word_labels[index]  \n",
        "        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n",
        "        \n",
        "        # step 2: add special tokens (and corresponding labels)\n",
        "        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n",
        "        labels.insert(0, \"O\") # add outside label for [CLS] token\n",
        "        labels.insert(-1, \"O\") # add outside label for [SEP] token\n",
        "\n",
        "        # step 3: truncating/padding\n",
        "        maxlen = self.max_len\n",
        "\n",
        "        if (len(tokenized_sentence) > maxlen):\n",
        "          # truncate\n",
        "          tokenized_sentence = tokenized_sentence[:maxlen]\n",
        "          labels = labels[:maxlen]\n",
        "        else:\n",
        "          # pad\n",
        "          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n",
        "          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n",
        "\n",
        "        # step 4: obtain the attention mask\n",
        "        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n",
        "        \n",
        "        # step 5: convert tokens to input ids\n",
        "        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n",
        "\n",
        "        label_ids = [label2id[label] for label in labels]\n",
        "        # the following line is deprecated\n",
        "        #label_ids = [label if label != 0 else -100 for label in label_ids]\n",
        "        \n",
        "        return {\n",
        "              'ids': torch.tensor(ids, dtype=torch.long),\n",
        "              'mask': torch.tensor(attn_mask, dtype=torch.long),\n",
        "              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n",
        "              'targets': torch.tensor(label_ids, dtype=torch.long)\n",
        "        } \n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.len"
      ],
      "metadata": {
        "id": "vAi5L8sao2R1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_size = 0.8\n",
        "train_dataset = data.sample(frac=train_size,random_state=200)\n",
        "test_dataset = data.drop(train_dataset.index).reset_index(drop=True)\n",
        "train_dataset = train_dataset.reset_index(drop=True)\n",
        "\n",
        "print(\"FULL Dataset: {}\".format(data.shape))\n",
        "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
        "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
        "\n",
        "training_set = dataset(train_dataset, tokenizer, MAX_LEN)\n",
        "testing_set = dataset(test_dataset, tokenizer, MAX_LEN)"
      ],
      "metadata": {
        "id": "urVPkbLLphwT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for token, label in zip(tokenizer.convert_ids_to_tokens(training_set[0][\"ids\"][:30]), training_set[0][\"targets\"][:30]):\n",
        "  print('{0:10}  {1}'.format(token, id2label[label.item()]))"
      ],
      "metadata": {
        "id": "tIPNdnbLeHot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_params = {'batch_size': TRAIN_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "test_params = {'batch_size': VALID_BATCH_SIZE,\n",
        "                'shuffle': True,\n",
        "                'num_workers': 0\n",
        "                }\n",
        "\n",
        "training_loader = DataLoader(training_set, **train_params)\n",
        "testing_loader = DataLoader(testing_set, **test_params)\n"
      ],
      "metadata": {
        "id": "CFkAHnqapp7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the model**"
      ],
      "metadata": {
        "id": "T-qAr1OjpsTO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForTokenClassification.from_pretrained('bert-base-uncased', # can change model here\n",
        "                                                   num_labels=len(id2label),\n",
        "                                                   id2label=id2label,\n",
        "                                                   label2id=label2id)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "3ZrOF0HIpuLs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Training**"
      ],
      "metadata": {
        "id": "Duj63rrqqA4E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids = training_set[0][\"ids\"].unsqueeze(0)\n",
        "mask = training_set[0][\"mask\"].unsqueeze(0)\n",
        "targets = training_set[0][\"targets\"].unsqueeze(0)\n",
        "ids = ids.to(device)\n",
        "mask = mask.to(device)\n",
        "targets = targets.to(device)\n",
        "outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "initial_loss = outputs[0]\n",
        "initial_loss"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rVwZzuRew9U",
        "outputId": "57dfef56-4da8-458b-b1f7-d6faac7617c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(2.7869, device='cuda:0', grad_fn=<NllLossBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tr_logits = outputs[1]\n",
        "tr_logits.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bYwfqZ4Beu8n",
        "outputId": "358f470e-d9a5-4b55-a348-1c3b75cf5afc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 128, 17])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "8iWVanMlp95v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(epoch):\n",
        "    tr_loss, tr_accuracy = 0, 0\n",
        "    nb_tr_examples, nb_tr_steps = 0, 0\n",
        "    tr_preds, tr_labels = [], []\n",
        "    # put model in training mode\n",
        "    model.train()\n",
        "\n",
        "    for idx, batch in enumerate(training_loader):\n",
        "        \n",
        "        ids = batch['ids'].to(device, dtype = torch.long)\n",
        "        mask = batch['mask'].to(device, dtype = torch.long)\n",
        "        targets = batch['targets'].to(device, dtype = torch.long)\n",
        "\n",
        "        outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "        loss, tr_logits = outputs.loss, outputs.logits\n",
        "        tr_loss += loss.item()\n",
        "\n",
        "        nb_tr_steps += 1\n",
        "        nb_tr_examples += targets.size(0)\n",
        "        \n",
        "        if idx % 100==0:\n",
        "            loss_step = tr_loss/nb_tr_steps\n",
        "            print(f\"Training loss per 100 training steps: {loss_step}\")\n",
        "\n",
        "        # compute training accuracy\n",
        "        flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "        # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
        "        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
        "        targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "        \n",
        "        tr_preds.extend(predictions)\n",
        "        tr_labels.extend(targets)\n",
        "        \n",
        "        tmp_tr_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "        tr_accuracy += tmp_tr_accuracy\n",
        "    \n",
        "        # gradient clipping\n",
        "        torch.nn.utils.clip_grad_norm_(\n",
        "            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n",
        "        )\n",
        "        \n",
        "        # backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    epoch_loss = tr_loss / nb_tr_steps\n",
        "    tr_accuracy = tr_accuracy / nb_tr_steps\n",
        "    print(f\"Training loss epoch: {epoch_loss}\")\n",
        "    print(f\"Training accuracy epoch: {tr_accuracy}\")"
      ],
      "metadata": {
        "id": "QRyvWVYNm972"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop for model\n",
        "for epoch in range(EPOCHS):\n",
        "    print(f\"Training epoch: {epoch + 1}\")\n",
        "    train(epoch)"
      ],
      "metadata": {
        "id": "e8ojTxqKqJwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate model**"
      ],
      "metadata": {
        "id": "g2eN1TIrqRWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def valid(model, testing_loader):\n",
        "    # put model in evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_examples, nb_eval_steps = 0, 0\n",
        "    eval_preds, eval_labels = [], []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for idx, batch in enumerate(testing_loader):\n",
        "            \n",
        "            ids = batch['ids'].to(device, dtype = torch.long)\n",
        "            mask = batch['mask'].to(device, dtype = torch.long)\n",
        "            targets = batch['targets'].to(device, dtype = torch.long)\n",
        "            \n",
        "            outputs = model(input_ids=ids, attention_mask=mask, labels=targets)\n",
        "            loss, eval_logits = outputs.loss, outputs.logits\n",
        "            \n",
        "            eval_loss += loss.item()\n",
        "\n",
        "            nb_eval_steps += 1\n",
        "            nb_eval_examples += targets.size(0)\n",
        "        \n",
        "            if idx % 100==0:\n",
        "                loss_step = eval_loss/nb_eval_steps\n",
        "                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n",
        "              \n",
        "            # compute evaluation accuracy\n",
        "            flattened_targets = targets.view(-1) # shape (batch_size * seq_len,)\n",
        "            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n",
        "            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n",
        "            # now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n",
        "            active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n",
        "            targets = torch.masked_select(flattened_targets, active_accuracy)\n",
        "            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n",
        "            \n",
        "            eval_labels.extend(targets)\n",
        "            eval_preds.extend(predictions)\n",
        "            \n",
        "            tmp_eval_accuracy = accuracy_score(targets.cpu().numpy(), predictions.cpu().numpy())\n",
        "            eval_accuracy += tmp_eval_accuracy\n",
        "    \n",
        "    #print(eval_labels)\n",
        "    #print(eval_preds)\n",
        "\n",
        "    labels = [id2label[id.item()] for id in eval_labels]\n",
        "    predictions = [id2label[id.item()] for id in eval_preds]\n",
        "\n",
        "    #print(labels)\n",
        "    #print(predictions)\n",
        "    \n",
        "    eval_loss = eval_loss / nb_eval_steps\n",
        "    eval_accuracy = eval_accuracy / nb_eval_steps\n",
        "    print(f\"Validation Loss: {eval_loss}\")\n",
        "    print(f\"Validation Accuracy: {eval_accuracy}\")\n",
        "\n",
        "    return labels, predictions"
      ],
      "metadata": {
        "id": "Bcp34yC1qTYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels, predictions = valid(model, testing_loader)"
      ],
      "metadata": {
        "id": "eo-FdVhJqV__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from seqeval.metrics import classification_report\n",
        "\n",
        "print(classification_report([labels], [predictions]))"
      ],
      "metadata": {
        "id": "WPwx1qVpqdFE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model weights and tokenizer\n",
        "model_path = \"/path/to/directory\"\n",
        "\n",
        "model.save_pretrained(model_path)\n",
        "tokenizer.save_pretrained(model_path)"
      ],
      "metadata": {
        "id": "gVHet0kG8vkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference**"
      ],
      "metadata": {
        "id": "TFJRHeXyA9Q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code below runs inference on the merged text boxes (each is referred to as a \"sentence\") using the HuggingFace Pipelines API - documentation is linked [here](https://huggingface.co/docs/transformers/main_classes/pipelines).\n",
        "\n",
        "This assumes that you have an input directory consisting of .txt files, and have an output directory that you want .csv files to exported to. The output is a csv file containing the labelled classes as columns (with each entity outputted to a separate column for the classes \"PER\", \"LOC\" and \"OCC\"), and each patent being recorded as a row (observation). You may need to modify this code depending on your desired output format.\n",
        "\n",
        "Running this code on a GPU is strongly recommended. A Nvidia Tesla T4 GPU (provided on the Colab free plan) is orders of magnitude faster than using the CPU. On the T4, inference on 1000 patents takes approximately 15-20 seconds, but several hours on the Colab CPU. "
      ],
      "metadata": {
        "id": "aolR5o6WBdO6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model weights and configuration\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_path)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)"
      ],
      "metadata": {
        "id": "7NlpycbeBFcR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set input and output directories\n",
        "input_dir = \"/path/to/input/dir\"\n",
        "output_dir = \"/path/to/output/dir\""
      ],
      "metadata": {
        "id": "6ikOIER9BO1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Deploy Pipeline API: device = 0 for GPU, device = -1 is default (for CPU)\n",
        "pipe = pipeline(task=\"token-classification\", model=model, device = 0, tokenizer=tokenizer, aggregation_strategy=\"simple\")"
      ],
      "metadata": {
        "id": "UjJA5hdXBTNq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop over all .txt files in input directory\n",
        "for filename in tqdm(os.listdir(input_dir)):\n",
        "    if filename.endswith(\".txt\"):\n",
        "        # Specify file paths\n",
        "        input_path = os.path.join(input_dir, filename)\n",
        "        output_path = os.path.join(output_dir, filename[:-4] + \".csv\")  # Remove .txt extension and add .csv extension\n",
        "\n",
        "        # Read sentences from text file\n",
        "        with open(input_path, \"r\") as f:\n",
        "            sentences = f.read().split(\"\\n\\n\")\n",
        "\n",
        "        # Create list of dictionaries to store entities for each sentence\n",
        "        all_entities = []\n",
        "        fieldnames = [\"NUM\", \"PER\", \"DATE\", \"LOC\", \"COMM\", \"OCC\", \"MISC\", \"INFO\"]  # Specify the fieldnames\n",
        "\n",
        "        for sentence in sentences:\n",
        "            # Extract entities\n",
        "            combined_entities = {}\n",
        "            for entity in pipe(sentence):\n",
        "                entity_group = entity['entity_group']\n",
        "                word = entity['word']\n",
        "                if entity_group not in combined_entities:\n",
        "                    combined_entities[entity_group] = []\n",
        "                combined_entities[entity_group].append(word)\n",
        "\n",
        "            # Create a new dictionary to store the updated entities\n",
        "            updated_entities = {}\n",
        "            for entity_group, words in combined_entities.items():\n",
        "                if entity_group in [\"PER\"]:\n",
        "                    for i, word in enumerate(words):\n",
        "                        column_name = f\"{entity_group}_{i + 1}\"\n",
        "                        if column_name not in fieldnames:\n",
        "                            updated_entities[column_name] = word\n",
        "                else:\n",
        "                    updated_entities[entity_group] = '& '.join(words)\n",
        "\n",
        "            # Add updated entities for this sentence to list\n",
        "            all_entities.append(updated_entities)\n",
        "\n",
        "        # Update fieldnames to include separate columns for PER, LOC, and OCC\n",
        "        max_columns = {key: 0 for key in [\"PER\"]}\n",
        "        for entity_dict in all_entities:\n",
        "            for key in max_columns.keys():\n",
        "                max_columns[key] = max(max_columns[key], len([k for k in entity_dict.keys() if k.startswith(key)]))\n",
        "\n",
        "        for key, count in max_columns.items():\n",
        "            for i in range(count):\n",
        "                column_name = f\"{key}_{i + 1}\"\n",
        "                if column_name not in fieldnames:\n",
        "                    fieldnames.append(column_name)\n",
        "\n",
        "        # Write entities to CSV file\n",
        "        with open(output_path, 'w', newline='') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(all_entities)"
      ],
      "metadata": {
        "id": "GYkuf07RBU9o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}